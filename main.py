import os
import shutil
import random
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm 
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F # Import F for functional operations
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_curve, auc, precision_recall_curve,  brier_score_loss, log_loss, f1_score, roc_auc_score  # ‚úÖ FIXED: Added f1_score
from scipy.special import softmax # For converting logits to probabilities
from sklearn.metrics.pairwise import cosine_similarity # For similarity-based confidence adjustment
from sklearn.isotonic import IsotonicRegression # For Isotonic Regression
from scipy.optimize import minimize # For Beta Calibration


# --- Configuration and Hyperparameters ---
class Config:
    def __init__(self):
        # Th∆∞ m·ª•c l√†m vi·ªác chung
        self.WORKING_DIR = os.path.abspath(os.getcwd())
        self.DATA_DIR = os.path.join(self.WORKING_DIR, '')
        os.makedirs(self.DATA_DIR, exist_ok=True)

        # C·∫•u h√¨nh c√°c t·∫≠p d·ªØ li·ªáu
        self.DATASETS = {
            'intracranial_hemorrhage': {
                'name': 'Intracranial Hemorrhage',
                'curl': "curl -L -o ./computed-tomography-ct-images.zip https://www.kaggle.com/api/v1/datasets/download/vbookshelf/computed-tomography-ct-images",
                'zip_path': os.path.join(self.DATA_DIR, 'computed-tomography-ct-images.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'computed-tomography-ct-images'),
                'label_1': 'Patients_CT',
                'label_2': 'hemorrhage_diagnosis.csv',
                'type': 'CT',
                'custom_dataset_class': 'HemorrhagicDataset'
            },
            'sarscov2_ct': {
                'name': 'SAR-CoV-2 CT',
                'curl': "curl -L -o ./sarscov2-ctscan-dataset.zip https://www.kaggle.com/api/v1/datasets/download/plameneduardo/sarscov2-ctscan-dataset",
                'zip_path': os.path.join(self.DATA_DIR, 'sarscov2-ctscan-dataset.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'sarscov2-ctscan-dataset'),
                'label_1': 'COVID',
                'label_2': 'non-COVID',
                'type': 'CT',
                'custom_dataset_class': None
            },
            'computed_tomography_brain': {
                'name': 'Computed Tomography Brain',
                'curl': "curl -L -o ./computed-tomography-ct-of-the-brain.zip https://www.kaggle.com/api/v1/datasets/download/trainingdatapro/computed-tomography-ct-of-the-brain",
                'zip_path': os.path.join(self.DATA_DIR, 'computed-tomography-ct-of-the-brain.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'sarscov2-ctscan-dataset'),
                'label_1': 'files/aneurysm',
                'label_2': 'files/cancer',
                'type': 'CT',
                'custom_dataset_class': None
            },
            'head_ct_hemorrhage': {
                'name': 'Head CT Hemorrhage',
                'curl': "curl -L -o ./head-ct-hemorrhage.zip https://www.kaggle.com/api/v1/datasets/download/felipekitamura/head-ct-hemorrhage",
                'zip_path': os.path.join(self.DATA_DIR, 'head-ct-hemorrhage.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'head-ct-hemorrhage'),
                'label_1': 'head_ct/head_ct',
                'label_2': 'labels.csv',
                'type': 'CT',
                'custom_dataset_class': 'HeadCTDataset'
            },
            'cleaned_mri_image': {
                'name': 'Cleaned MRI Image',
                'curl': "curl -L -o ./mri-image-data.zip https://www.kaggle.com/api/v1/datasets/download/alaminbhuyan/mri-image-data",
                'zip_path': os.path.join(self.DATA_DIR, 'mri-image-data.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'mri-image-data'),
                'label_1': 'CleandMRIImageData/Training/glioma',
                'label_2': 'CleandMRIImageData/Training/meningioma',
                'type': 'MRI',
                'custom_dataset_class': None
            },
            'brain_tumor_mri': {
                'name': 'Brain Tumor MRI',
                'curl': "curl -L -o ./brain-tumor-mri-dataset.zip https://www.kaggle.com/api/v1/datasets/download/masoudnickparvar/brain-tumor-mri-dataset",
                'zip_path': os.path.join(self.DATA_DIR, 'brain-tumor-mri-dataset.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'brain-tumor-mri-dataset'),
                'label_1': 'Training/glioma',
                'label_2': 'Training/meningioma',
                'type': 'MRI',
                'custom_dataset_class': None
            },
            'brain_cancer_mri': {
                'name': 'Brain Cancer MRI',
                'curl': "curl -L -o ./brain-cancer-mri-dataset.zip https://www.kaggle.com/api/v1/datasets/download/orvile/brain-cancer-mri-dataset",
                'zip_path': os.path.join(self.DATA_DIR, 'brain-cancer-mri-dataset.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'brain-cancer-mri-dataset'),
                'label_1': 'Brain_Cancer raw MRI data/Brain_Cancer/brain_glioma',
                'label_2': 'Brain_Cancer raw MRI data/Brain_Cancer/brain_menin',
                'type': 'MRI',
                'custom_dataset_class': None
            },
            'breast_cancer_mri': {
                'name': 'Breast Cancer MRI',
                'curl': "curl -L -o ./breast-cancer-patients-mris.zip https://www.kaggle.com/api/v1/datasets/download/uzairkhan45/breast-cancer-patients-mris",
                'zip_path': os.path.join(self.DATA_DIR, 'breast-cancer-patients-mris.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'breast-cancer-patients-mris'),
                'label_1': "Breast Cancer Patients MRI's/train/Healthy",
                'label_2': "Breast Cancer Patients MRI's/train/Sick",
                'type': 'MRI',
                'custom_dataset_class': None
            },
            'covidqu_xray': {
                'name': 'Covid-qu-ex X-ray',
                'curl': "curl -L -o ./covidqu.zip https://www.kaggle.com/api/v1/datasets/download/anasmohammedtahir/covidqu",
                'zip_path': os.path.join(self.DATA_DIR, 'covidqu.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'covidqu'),
                'label_1': 'Infection Segmentation Data/Infection Segmentation Data/Train/COVID-19/images',
                'label_2': 'Infection Segmentation Data/Infection Segmentation Data/Train/Normal/images',
                'type': 'X-ray',
                'custom_dataset_class': None
            },
            'chest_xray_pneumonia': {
                'name': 'Chest X-Ray Pneumonia',
                'curl': "curl -L -o ./chest-xray-pneumonia.zip https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/chest-xray-pneumonia",
                'zip_path': os.path.join(self.DATA_DIR, 'chest-xray-pneumonia.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'chest-xray-pneumonia'),
                'label_1': 'chest-xray-pneumonia/train/NORMAL',
                'label_2': 'chest-xray-pneumonia/train/PNEUMONIA',
                'type': 'X-ray',
                'custom_dataset_class': None
            },
            'tuberculosis_xray': {
                'name': 'Tuberculosis X-ray',
                'curl': "curl -L -o ./tuberculosis-tb-chest-xray-dataset.zip https://www.kaggle.com/api/v1/datasets/download/tawsifurrahman/tuberculosis-tb-chest-xray-dataset",
                'zip_path': os.path.join(self.DATA_DIR, 'tuberculosis-tb-chest-xray-dataset.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'tuberculosis-tb-chest-xray-dataset'),
                'label_1': 'TB_Chest_Radiography_Database/Normal',
                'label_2': 'TB_Chest_Radiography_Database/Tuberculosis',
                'type': 'X-ray',
                'custom_dataset_class': None
            },
            'covid19_radiography': {
                'name': 'COVID-19 Radiography',
                'curl': "curl -L -o ./covid19-radiography-database.zip https://www.kaggle.com/api/v1/datasets/download/tawsifurrahman/covid19-radiography-database",
                'zip_path': os.path.join(self.DATA_DIR, 'covid19-radiography-database.zip'),
                'extract_path': os.path.join(self.DATA_DIR, 'covid19-radiography-database'),
                'label_1': 'COVID-19_Radiography_Dataset/Normal/images',
                'label_2': 'COVID-19_Radiography_Dataset/COVID/images',
                'type': 'X-ray',
                'custom_dataset_class': None
            }
        }

        # Th∆∞ m·ª•c ph√¢n lo·∫°i
        self.COVID_DIR = os.path.join(self.WORKING_DIR, 'UNNORMAL')  # Th∆∞ m·ª•c cho nh√£n t√≠ch c·ª±c
        self.NON_COVID_DIR = os.path.join(self.WORKING_DIR, 'NORMAL')  # Th∆∞ m·ª•c cho nh√£n ti√™u c·ª±c
        os.makedirs(self.COVID_DIR, exist_ok=True)
        os.makedirs(self.NON_COVID_DIR, exist_ok=True)

        # C·∫•u h√¨nh m√¥ h√¨nh v√† hu·∫•n luy·ªán
        self.IMAGE_SIZE = (224, 224)  # K√≠ch th∆∞·ªõc chu·∫©n cho nhi·ªÅu CNN
        self.BATCH_SIZE = 16
        self.NUM_EPOCHS_PER_MODEL = 2
        self.LEARNING_RATE = 1e-4
        self.NUM_ENSEMBLE_MODELS = 3

        # C·∫•u h√¨nh Monte Carlo Dropout (MCDO)
        self.MCDO_ENABLE = False       # B·∫≠t/t·∫Øt Monte Carlo Dropout (Baseline A.1)
        self.MCDO_DROPOUT_RATE = 0.5   # T·ª∑ l·ªá dropout cho MCDO
        self.MCDO_NUM_RUNS = 10        # ƒê√£ gi·∫£m s·ªë l·∫ßn ch·∫°y forward pass cho MCDO ƒë·ªÉ ∆∞·ªõc t√≠nh ƒë·ªô b·∫•t ƒë·ªãnh

        # C·∫•u h√¨nh Label Smoothing (Baseline A.2.3)
        self.LABEL_SMOOTHING_ENABLE = False # B·∫≠t/t·∫Øt Label Smoothing
        self.LABEL_SMOOTHING_EPSILON = 0.1 # Tham s·ªë epsilon cho Label Smoothing

        # C·∫•u h√¨nh Training Dynamics (Baseline B.3)
        self.ENABLE_TRAINING_DYNAMICS = False # C·ªù b·∫≠t/t·∫Øt ƒëi·ªÅu ch·ªânh ƒë·ªô tin c·∫≠y b·∫±ng training dynamics
        self.TRAINING_DYNAMICS_CONF_PENALTY = 0.1 # M·ª©c ƒë·ªô c√°c v√≠ d·ª• "kh√≥ h·ªçc" trong qu√° tr√¨nh hu·∫•n luy·ªán l√†m gi·∫£m ƒë·ªô tin c·∫≠y c·ªßa c√°c v√≠ d·ª• test t∆∞∆°ng t·ª±
        
        # ODIN/Energy Score Parameters (Baseline B.1.x, B.2.x)
        self.ODIN_TEMP = 1000.0 # Nhi·ªát ƒë·ªô cho ODIN. Nhi·ªát ƒë·ªô cao th∆∞·ªùng ho·∫°t ƒë·ªông t·ªët.
        self.ODIN_EPSILON = 0.001 # ƒê·ªô l·ªõn nhi·ªÖu lo·∫°n cho ODIN (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh d·ª±a tr√™n t·∫≠p d·ªØ li·ªáu)
        self.ENERGY_CLASSIFY_PERCENTILE_THRESHOLD = 20 # C√°c m·∫´u v·ªõi ƒëi·ªÉm nƒÉng l∆∞·ª£ng trong X% th·∫•p nh·∫•t ƒë∆∞·ª£c coi l√† OOD ti·ªÅm nƒÉng
        
        # M·ª•c ti√™u t·ª´ ch·ªëi
        self.TARGET_ACCEPTED_ACCURACY = 0.99 # 99.5% ƒë·ªô ch√≠nh x√°c tr√™n c√°c tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c ch·∫•p nh·∫≠n
        self.TARGET_REJECTION_RATE = 0.05      # T·ª´ ch·ªëi kho·∫£ng 5% c√°c tr∆∞·ªùng h·ª£p

        # C√°c tham s·ªë c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh th·ªß c√¥ng cho ph√¢n lo·∫°i ch·ªçn l·ªçc (th·ª≠ nghi·ªám v·ªõi ch√∫ng!)
        self.DISAGREEMENT_PENALTY_FACTOR = 5.0 # M·ª©c ƒë·ªô b·∫•t ƒë·ªìng c·ªßa ensemble l√†m gi·∫£m ƒë·ªô tin c·∫≠y
        
        # Tr·ªçng s·ªë cho m·ª•c ti√™u t·ªëi ∆∞u h√≥a ng∆∞·ª°ng t·ª´ ch·ªëi (m·ª©c ƒë·ªô quan tr·ªçng c·ªßa m·ªói y·∫øu t·ªë)
        # C√°c tr·ªçng s·ªë n√†y cho ph√©p ƒëi·ªÅu ch·ªânh s·ª± ƒë√°nh ƒë·ªïi gi·ªØa ƒë·ªô ch√≠nh x√°c ƒë∆∞·ª£c ch·∫•p nh·∫≠n, t·ª∑ l·ªá t·ª´ ch·ªëi v√† ECE
        self.ACCURACY_DEVIATION_WEIGHT = 2.5 # Tr·ªçng s·ªë cao ƒë·ªÉ m·∫°nh m·∫Ω th·ª±c thi TARGET_ACCEPTED_ACCURACY
        self.REJECTION_RATE_DEVIATION_WEIGHT = 1.0 # Tr·ªçng s·ªë chu·∫©n cho ƒë·ªô l·ªách t·ª∑ l·ªá t·ª´ ch·ªëi
        self.ECE_DEVIATION_WEIGHT = 5.0       # Tr·ªçng s·ªë cho ECE trong t·ªëi ∆∞u h√≥a ng∆∞·ª°ng t·ª´ ch·ªëi (ƒëi·ªÅu ch·ªânh c√°i n√†y, gi√° tr·ªã cao h∆°n c√≥ nghƒ©a l√† t·∫≠p ch·∫•p nh·∫≠n ƒë∆∞·ª£c hi·ªáu ch·ªânh t·ªët h∆°n)

        # Ng∆∞·ª°ng ph√°t hi·ªán OOD (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh th·ªß c√¥ng trong categorize_rejected_cases)
        # ƒê√¢y l√† c√°c ng∆∞·ª°ng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ PH√ÇN LO·∫†I c√°c tr∆∞·ªùng h·ª£p t·ª´ ch·ªëi, KH√îNG ph·∫£i ƒë·ªÉ ra quy·∫øt ƒë·ªãnh t·ª´ ch·ªëi ban ƒë·∫ßu
        self.OOD_CONFIDENCE_THRESHOLD = 0.65 # C√°c m·∫´u d∆∞·ªõi ƒë·ªô tin c·∫≠y n√†y
        self.OOD_VARIANCE_THRESHOLD = 0.08  # V√† tr√™n ph∆∞∆°ng sai n√†y l√† OOD ti·ªÅm nƒÉng (d√πng cho c√°c baseline c≈©)
        self.ODIN_CLASSIFY_THRESHOLD = 0.8 # C√°c m·∫´u v·ªõi ƒëi·ªÉm ODIN < ng∆∞·ª°ng n√†y ƒë∆∞·ª£c coi l√† OOD ti·ªÅm nƒÉng
        # B·∫≠t/t·∫Øt Weighted Logits theo ƒê·ªô tin c·∫≠y (ECE)
        self.USE_WEIGHTED_ENSEMBLE = True 
        
        # B·∫≠t/t·∫Øt Dynamic Ensemble Selection (ch·ªâ ch·ªçn model t·ªët nh·∫•t cho t·ª´ng m·∫´u)
        # L∆∞u √Ω: ƒê·∫∑t NUM_ENSEMBLE_MODELS = 3 ƒë·ªÉ logic ch·ªçn 2/3 ho·∫°t ƒë·ªông ƒë√∫ng
        self.USE_DYNAMIC_SELECTION = True
        self.DYNAMIC_SELECTION_COUNT = 2 # Ch·ªçn 2 model t·ªët nh·∫•t

        # H·∫±ng s·ªë nh·ªè ƒë·ªÉ tr√°nh chia cho 0 khi t√≠nh tr·ªçng s·ªë t·ª´ ECE
        self.EPSILON_ECE = 1e-8

        # C√°c c√†i ƒë·∫∑t kh√°c
        self.RANDOM_SEED = 42
        self.DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.MODEL_SAVE_DIR = 'working/models' # Th∆∞ m·ª•c ƒë·ªÉ l∆∞u c√°c m√¥ h√¨nh ensemble ƒë√£ hu·∫•n luy·ªán
        os.makedirs(self.MODEL_SAVE_DIR, exist_ok=True)
        self.XAI_SAVE_DIR = 'working/xai_visualizations' # Th∆∞ m·ª•c ƒë·ªÉ l∆∞u c√°c h√¨nh ·∫£nh XAI
        os.makedirs(self.XAI_SAVE_DIR, exist_ok=True) # ƒê·∫£m b·∫£o th∆∞ m·ª•c ƒë·∫ßu ra XAI t·ªìn t·∫°i

        self.ODIN_TEMPERATURE = 1000  # Temperature for ODIN
        self.ODIN_UPDATE_PREDICTIONS = False  # Whether to update predictions based on ODIN perturbed probs
        self.COMBINE_OOD_WITH_DISAGREEMENT = True  # Combine ODIN scores with disagreement penalty
        self.ODIN_THRESHOLD = 0.5  # Threshold for OOD detection (adjust based on validation)

cfg = Config()

# Thi·∫øt l·∫≠p seed ng·∫´u nhi√™n ƒë·ªÉ t√°i s·∫£n xu·∫•t k·∫øt qu·∫£
def set_seed(seed):
    """ƒê·∫∑t seed ng·∫´u nhi√™n ƒë·ªÉ t√°i s·∫£n xu·∫•t k·∫øt qu·∫£ tr√™n c√°c th∆∞ vi·ªán kh√°c nhau."""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(cfg.RANDOM_SEED)

print(f"S·ª≠ d·ª•ng thi·∫øt b·ªã: {cfg.DEVICE}")

# --- 1. T·∫£i v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ---
class CTScanDataset(Dataset):
    """
    L·ªõp Dataset t√πy ch·ªânh ƒë·ªÉ t·∫£i h√¨nh ·∫£nh CT scan v√† nh√£n c·ªßa ch√∫ng.
    X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n h√¨nh ·∫£nh v√† √°p d·ª•ng c√°c ph√©p bi·∫øn ƒë·ªïi.
    Tr·∫£ v·ªÅ h√¨nh ·∫£nh, nh√£n, v√† ch·ªâ m·ª•c to√†n c·ª•c g·ªëc c·ªßa n√≥.
    """
    def __init__(self, image_paths, labels, transform=None, global_indices=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        # ƒê·∫£m b·∫£o global_indices l√† m·ªôt m·∫£ng numpy
        self.global_indices = np.array(global_indices) if global_indices is not None else np.arange(len(image_paths))
        # T·∫°o √°nh x·∫° t·ª´ global_idx ƒë·∫øn local idx trong ph√¢n t√°ch n√†y ƒë·ªÉ tra c·ª©u thu·∫≠n ti·ªán
        self.original_indices_map = {self.global_indices[i]: i for i in range(len(self.global_indices))}

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB') # ƒê·∫£m b·∫£o 3 k√™nh cho c√°c m√¥ h√¨nh ti·ªÅn hu·∫•n luy·ªán
        label = self.labels[idx]
        global_idx = self.global_indices[idx] # Tr·∫£ v·ªÅ ch·ªâ m·ª•c to√†n c·ª•c g·ªëc

        if self.transform:
            image = self.transform(image)

        return image, label, global_idx # Tr·∫£ v·ªÅ ch·ªâ m·ª•c to√†n c·ª•c g·ªëc ƒë·ªÉ theo d√µi ƒë·ªông l·ª±c hu·∫•n luy·ªán

class HemorrhagicDataset(Dataset):
    def __init__(self, root_dir, csv_file, transform=None, train=True, split_ratio=0.7):
        self.root_dir = root_dir
        self.transform = transform
        self.diagnosis = pd.read_csv(csv_file)
        
        # Gather all patient directories
        self.all_patients = sorted(os.listdir(os.path.join(self.root_dir, "Patients_CT")))
        
        # Split the patient directories into train and validation sets
        split_index = int(len(self.all_patients) * split_ratio)
        if train:
            self.patient_set = self.all_patients[:split_index]
        else:
            self.patient_set = self.all_patients[split_index:]
        
        self.slices = self._gather_slices()

    def _gather_slices(self):
        slices = []
        patients_dir = os.path.join(self.root_dir, "Patients_CT")
        for patient_number in os.listdir(patients_dir):
            patient_dir = os.path.join(patients_dir, patient_number, "brain")
            if os.path.exists(patient_dir):
                for file_name in os.listdir(patient_dir):
                    if file_name.endswith(".jpg") and "_HGE_Seg" not in file_name:
                        slice_number = file_name.split('.')[0]
                        slices.append((patient_dir, patient_number, slice_number))
        return slices

    def __len__(self):
        return len(self.slices)

    def __getitem__(self, idx):
        patient_dir, patient_number, slice_number = self.slices[idx]
        image_path = os.path.join(patient_dir, f"{slice_number}.jpg")
        mask_path = os.path.join(patient_dir, f"{slice_number}_HGE_Seg.jpg")

        # Load the image and mask
        image = Image.open(image_path).convert("L")
        if os.path.exists(mask_path):
            mask = Image.open(mask_path).convert("L")
        else:
            mask = Image.new("L", image.size)

        diag_row = self.diagnosis[(self.diagnosis['PatientNumber'] == int(patient_number))
                                  & (self.diagnosis['SliceNumber'] == int(slice_number))]
        label = "hemorrhagic" if not diag_row.empty and diag_row.iloc[0]['No_Hemorrhage'] == 0 else "normal"

        # mask = (mask != 0).astype(np.float32)
        
        # Convert images and masks to PyTorch tensors
        image = transforms.ToTensor()(image)
        mask = transforms.ToTensor()(mask)

        
        sample = {'image': image, 'mask': mask, 'label': label, 'original_type': 'image'}

        if self.transform:
            sample['image'] = self.transform(sample['image'])
            sample['mask'] = self.transform(sample['mask'])

        return sample

def organize_data(labels_df, data_dir, covid_dir, non_covid_dir):
    for index, row in labels_df.iterrows():
        img_id = row['id']
        hemorrhage = row['hemorrhage']
        img_filename = f"{img_id:03d}.png"  # ƒê·ªãnh d·∫°ng t√™n file ·∫£nh (001.jpg, 002.jpg, ...)
        src_path = os.path.join(data_dir, img_filename)
        
        if not os.path.exists(src_path):
            print(f"Kh√¥ng t√¨m th·∫•y file: {src_path}")
            continue
        
        if hemorrhage == 1:
            dest_path = os.path.join(covid_dir, img_filename)
        else:
            dest_path = os.path.join(non_covid_dir, img_filename)
        
        try:
            shutil.copy(src_path, dest_path)  # Sao ch√©p file
            print(f"ƒê√£ sao ch√©p {img_filename} ƒë·∫øn {dest_path}")
        except Exception as e:
            print(f"L·ªói khi sao ch√©p {img_filename}: {str(e)}")

def classify_images_to_dirs(dataset, covid_dir, non_covid_dir):
    """
    Ph√¢n lo·∫°i h√¨nh ·∫£nh t·ª´ HemorrhagicDataset v√†o hai th∆∞ m·ª•c d·ª±a tr√™n nh√£n.
    
    Args:
        dataset: HemorrhagicDataset instance
        covid_dir: Th∆∞ m·ª•c ƒë√≠ch cho h√¨nh ·∫£nh "hemorrhagic"
        non_covid_dir: Th∆∞ m·ª•c ƒë√≠ch cho h√¨nh ·∫£nh "normal"
    """
    for idx in tqdm(range(len(dataset)), desc="Classifying images"):
        sample = dataset[idx]
        image_path = os.path.join(dataset.slices[idx][0], f"{dataset.slices[idx][2]}.jpg")
        label = sample['label']
        
        # X√°c ƒë·ªãnh th∆∞ m·ª•c ƒë√≠ch d·ª±a tr√™n nh√£n
        if label == "hemorrhagic":
            dest_dir = covid_dir
        else:
            dest_dir = non_covid_dir
            
        # T·∫°o t√™n file ƒë√≠ch
        patient_number = dataset.slices[idx][1]
        slice_number = dataset.slices[idx][2]
        dest_filename = f"{patient_number}_{slice_number}.jpg"
        dest_path = os.path.join(dest_dir, dest_filename)
        
        # Sao ch√©p ho·∫∑c t·∫°o li√™n k·∫øt m·ªÅm t·ªõi th∆∞ m·ª•c ƒë√≠ch
        shutil.copy(image_path, dest_path)  # S·ª≠ d·ª•ng shutil.copy ƒë·ªÉ sao ch√©p file
        # N·∫øu b·∫°n mu·ªën ti·∫øt ki·ªám dung l∆∞·ª£ng, c√≥ th·ªÉ d√πng li√™n k·∫øt m·ªÅm:
        # os.symlink(image_path, dest_path)

def prepare_datasets(cfg, dataset_name):
    dataset_config = cfg.DATASETS[dataset_name]
    extract_path = dataset_config['extract_path']
    label_1 = dataset_config['label_1']
    label_2 = dataset_config['label_2']
    custom_dataset_class = dataset_config.get('custom_dataset_class')

    # X·ª≠ l√Ω t·∫≠p d·ªØ li·ªáu t√πy ch·ªânh
    if custom_dataset_class == 'HemorrhagicDataset':
        transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        hemo_dataset = HemorrhagicDataset(
            root_dir=extract_path,
            csv_file=os.path.join(extract_path, label_2),
            transform=transform,
            train=True
        )
        hemo_val_dataset = HemorrhagicDataset(
            root_dir=extract_path,
            csv_file=os.path.join(extract_path, label_2),
            transform=transform,
            train=False
        )
        classify_images_to_dirs(hemo_val_dataset, cfg.COVID_DIR, cfg.NON_COVID_DIR)
    elif custom_dataset_class == 'HeadCTDataset':
        labels_df = pd.read_csv(os.path.join(extract_path, label_2))
        labels_df.rename(columns={' hemorrhage': 'hemorrhage'}, inplace=True)
        organize_data(labels_df, os.path.join(extract_path, label_1), cfg.COVID_DIR, cfg.NON_COVID_DIR)
    else:
        # S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n label_1 v√† label_2 tr·ª±c ti·∫øp
        cfg.COVID_DIR = os.path.join(extract_path, label_1)
        cfg.NON_COVID_DIR = os.path.join(extract_path, label_2)

    # Thu th·∫≠p ƒë∆∞·ªùng d·∫´n v√† nh√£n
    all_image_paths_raw = []
    all_labels_raw = []
    covid_paths = [os.path.join(cfg.COVID_DIR, f) for f in os.listdir(cfg.COVID_DIR) if f.endswith(('.png', '.jpg'))]
    all_image_paths_raw.extend(covid_paths)
    all_labels_raw.extend([1] * len(covid_paths))
    non_covid_paths = [os.path.join(cfg.NON_COVID_DIR, f) for f in os.listdir(cfg.NON_COVID_DIR) if f.endswith(('.png', '.jpg'))]
    all_image_paths_raw.extend(non_covid_paths)
    all_labels_raw.extend([0] * len(non_covid_paths))

    # Thu th·∫≠p h√¨nh ·∫£nh COVID
    covid_paths = [os.path.join(cfg.COVID_DIR, f) for f in os.listdir(cfg.COVID_DIR) if f.endswith('.png') or f.endswith('.jpg')]
    all_image_paths_raw.extend(covid_paths)
    all_labels_raw.extend([1] * len(covid_paths)) # 1 cho COVID

    # Thu th·∫≠p h√¨nh ·∫£nh Non-COVID
    non_covid_paths = [os.path.join(cfg.NON_COVID_DIR, f) for f in os.listdir(cfg.NON_COVID_DIR) if f.endswith('.png') or f.endswith('.jpg')]
    all_image_paths_raw.extend(non_covid_paths)
    all_labels_raw.extend([0] * len(non_covid_paths)) # 0 cho non-COVID

    # G√°n ch·ªâ m·ª•c to√†n c·ª•c
    all_global_indices = list(range(len(all_image_paths_raw)))

    print(f"T·ªïng s·ªë h√¨nh ·∫£nh t√¨m th·∫•y: {len(all_image_paths_raw)}")
    print(f"H√¨nh ·∫£nh COVID: {len(covid_paths)}, H√¨nh ·∫£nh Non-COVID: {len(non_covid_paths)}")

    # T·∫°o c√°c ph√¢n t√°ch ph√¢n t·∫ßng cho t·∫≠p hu·∫•n luy·ªán+validation v√† t·∫≠p test tr∆∞·ªõc
    # Mong mu·ªën: Test = 15% t·ªïng s·ªë.
    train_val_paths, test_paths, train_val_labels, test_labels, \
    train_val_global_indices, test_global_indices = train_test_split(
        all_image_paths_raw, all_labels_raw, all_global_indices,
        test_size=0.15, random_state=cfg.RANDOM_SEED, stratify=all_labels_raw
    )
    
    # Sau ƒë√≥, chia t·∫≠p train_val th√†nh c√°c t·∫≠p hu·∫•n luy·ªán v√† validation th·ª±c t·∫ø
    # train_val l√† 85% t·ªïng s·ªë. Ch√∫ng ta mu·ªën Val = 15% t·ªïng s·ªë.
    # V√¨ v·∫≠y, val_size_relative_to_train_val = 0.15 / (1.0 - 0.15)
    val_size_relative_to_train_val = 0.15 / (1.0 - 0.15)
    train_paths, val_paths, train_labels, val_labels, \
    train_global_indices, val_global_indices = train_test_split(
        train_val_paths, train_val_labels, train_val_global_indices,
        test_size=val_size_relative_to_train_val, random_state=cfg.RANDOM_SEED, stratify=train_val_labels
    )

    print(f"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán: {len(train_paths)}")
    print(f"K√≠ch th∆∞·ªõc t·∫≠p Validation: {len(val_paths)}")
    print(f"K√≠ch th∆∞·ªõc t·∫≠p Test: {len(test_paths)}") # test_paths n√†y tham chi·∫øu ƒë·∫øn t·∫≠p test cu·ªëi c√πng, ƒë·ªôc l·∫≠p

    # ƒê·ªãnh nghƒ©a c√°c ph√©p bi·∫øn ƒë·ªïi
    train_transform = transforms.Compose([
        transforms.Resize(cfg.IMAGE_SIZE),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Chu·∫©n h√≥a ImageNet
    ])

    val_test_transform = transforms.Compose([
        transforms.Resize(cfg.IMAGE_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Truy·ªÅn global_indices v√†o constructor c·ªßa Dataset
    train_dataset = CTScanDataset(train_paths, train_labels, train_transform, train_global_indices)
    val_dataset = CTScanDataset(val_paths, val_labels, val_test_transform, val_global_indices)
    test_dataset = CTScanDataset(test_paths, test_labels, val_test_transform, test_global_indices)

    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=2)

    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset

# --- 2. ƒê·ªãnh nghƒ©a Base Classifier ---
class BaseClassifier(nn.Module):
    """
    M·ªôt b·ªô ph√¢n lo·∫°i CNN c∆° b·∫£n s·ª≠ d·ª•ng m√¥ h√¨nh ResNet ti·ªÅn hu·∫•n luy·ªán.
    Bao g·ªìm m·ªôt b·ªô tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng r√µ r√†ng ƒë·ªÉ d·ªÖ d√†ng k·∫øt n·ªëi cho Grad-CAM
    v√† ƒë·ªÉ l·∫•y ƒë·∫∑c tr∆∞ng cho ph√¢n t√≠ch d·ª±a tr√™n s·ª± t∆∞∆°ng ƒë·ªìng.
    T√≠ch h·ª£p c√°c l·ªõp Dropout cho Monte Carlo Dropout (MCDO).
    """
    def __init__(self, num_classes=2, dropout_rate=0.0):
        super(BaseClassifier, self).__init__()
        self.dropout_rate = dropout_rate
        # T·∫£i m√¥ h√¨nh ResNet18 ti·ªÅn hu·∫•n luy·ªán
        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        
        # X√°c ƒë·ªãnh l·ªõp m·ª•c ti√™u cho Grad-CAM (l·ªõp t√≠ch ch·∫≠p cu·ªëi c√πng)
        self.target_layer = self.model.layer4[-1] 
        
        # ƒê·ªãnh nghƒ©a ph·∫ßn tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (m·ªçi th·ª© tr∆∞·ªõc l·ªõp FC cu·ªëi c√πng)
        feature_extractor_layers = list(self.model.children())[:-1]
        
        # Th√™m l·ªõp dropout n·∫øu dropout_rate d∆∞∆°ng
        if self.dropout_rate > 0:
            # T√¨m ch·ªâ m·ª•c c·ªßa l·ªõp AdaptiveAvgPool2d ƒë·ªÉ ch√®n dropout sau n√≥
            try:
                avgpool_idx = [i for i, layer in enumerate(feature_extractor_layers) if isinstance(layer, nn.AdaptiveAvgPool2d)][0]
                feature_extractor_layers.insert(avgpool_idx + 1, nn.Dropout(p=self.dropout_rate))
                print(f"ƒê√£ th√™m l·ªõp Dropout v·ªõi t·ª∑ l·ªá {self.dropout_rate} v√†o b·ªô tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng.")
            except IndexError:
                print("C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y l·ªõp AdaptiveAvgPool2d. ƒêang th√™m Dropout sau t·∫•t c·∫£ c√°c l·ªõp t√≠ch ch·∫≠p.")
                feature_extractor_layers.append(nn.Dropout(p=self.dropout_rate))


        self.feature_extractor = nn.Sequential(*feature_extractor_layers)
        
        # Thay th·∫ø l·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß cu·ªëi c√πng cho ph√¢n lo·∫°i nh·ªã ph√¢n
        num_ftrs = self.model.fc.in_features
        self.model.fc = nn.Linear(num_ftrs, num_classes)

    def forward(self, x):
        # Forward pass qua b·ªô tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
        features = self.feature_extractor(x)
        features = torch.flatten(features, 1) # L√†m ph·∫≥ng c√°c ƒë·∫∑c tr∆∞ng
        # Forward pass qua l·ªõp ph√¢n lo·∫°i cu·ªëi c√πng
        output = self.model.fc(features)
        return output

    def get_features(self, x):
        """
        Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ l·ªõp tr∆∞·ªõc ƒë·∫ßu ph√¢n lo·∫°i cu·ªëi c√πng.
        """
        # L∆∞u √Ω: C√°c l·ªõp Dropout trong feature_extractor s·∫Ω t·ª± ƒë·ªông t·∫Øt n·∫øu model.eval()
        # ho·∫∑c ho·∫°t ƒë·ªông n·∫øu model.train() v√† dropout_rate > 0
        with torch.no_grad(): 
            features = self.feature_extractor(x)
            features = torch.flatten(features, 1) 
        return features


# --- Label Smoothing Loss (Baseline A.2.3) ---
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, epsilon=0.1, dim=-1):
        super(LabelSmoothingLoss, self).__init__()
        self.confidence = 1.0 - epsilon
        self.epsilon = epsilon
        self.cls = classes
        self.dim = dim

    def forward(self, pred, target):
        pred = pred.log_softmax(dim=self.dim)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.epsilon / (self.cls - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))

# --- 3. H√†m hu·∫•n luy·ªán cho c√°c th√†nh vi√™n Ensemble v·ªõi theo d√µi ƒë·ªông l·ª±c hu·∫•n luy·ªán ---
def train_model(model, train_loader, val_loader, epochs, lr, device, model_idx, cfg):
    """
    Hu·∫•n luy·ªán m·ªôt th·ªÉ hi·ªán duy nh·∫•t c·ªßa b·ªô ph√¢n lo·∫°i c∆° b·∫£n v√† theo d√µi ƒë·ªông l·ª±c hu·∫•n luy·ªán chi ti·∫øt
    (nh√£n d·ª± ƒëo√°n v√† ƒë·ªô tin c·∫≠y cho m·ªói m·∫´u ·ªü m·ªói epoch).
    """
    if cfg.LABEL_SMOOTHING_ENABLE:
        criterion = LabelSmoothingLoss(classes=2, epsilon=cfg.LABEL_SMOOTHING_EPSILON).to(device)
        print(f"ƒê√£ b·∫≠t Label Smoothing v·ªõi epsilon: {cfg.LABEL_SMOOTHING_EPSILON}")
    else:
        criterion = nn.CrossEntropyLoss().to(device)

    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)

    model.to(device)
    best_val_accuracy = 0.0
    
    # T·ª´ ƒëi·ªÉn ƒë·ªÉ l∆∞u tr·ªØ l·ªãch s·ª≠ d·ª± ƒëo√°n chi ti·∫øt cho m·ªói m·∫´u hu·∫•n luy·ªán qua c√°c epoch
    # Key: global_idx c·ªßa m·∫´u, Value: Danh s√°ch c√°c dict, m·ªói dict (epoch, is_correct, predicted_label, confidence)
    training_prediction_details = {global_idx: [] for global_idx in train_loader.dataset.global_indices}


    print(f"\n--- Hu·∫•n luy·ªán M√¥ h√¨nh Ensemble {model_idx + 1} ---")
    for epoch in range(epochs):
        model.train() # ƒê·∫£m b·∫£o m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô train (dropout ho·∫°t ƒë·ªông n·∫øu c√≥)
        running_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        for inputs, labels, global_indices_batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} (Train)", dynamic_ncols=True):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            
            # T√°ch c√°c outputs tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi sang numpy
            probs = softmax(outputs.detach().cpu().numpy(), axis=1) 
            predicted_labels = np.argmax(probs, axis=1)
            confidences_batch = np.max(probs, axis=1)

            total_samples += labels.size(0)
            correct_in_batch = (predicted_labels == labels.cpu().numpy()).sum().item()
            correct_predictions += correct_in_batch

            # C·∫≠p nh·∫≠t ƒë·ªông l·ª±c hu·∫•n luy·ªán: l∆∞u nh√£n d·ª± ƒëo√°n v√† ƒë·ªô tin c·∫≠y c·ªßa n√≥ cho m·ªói m·∫´u ·ªü epoch n√†y
            for i, global_idx_tensor in enumerate(global_indices_batch):
                global_idx = global_idx_tensor.item()
                is_correct_prediction = (predicted_labels[i] == labels[i].item())
                training_prediction_details[global_idx].append({
                    'epoch': epoch,
                    'is_correct': is_correct_prediction,
                    'predicted_label': predicted_labels[i],
                    'confidence': confidences_batch[i]
                })
        
        # Clear CUDA cache after each epoch to free up memory
        if device.type == 'cuda':
            torch.cuda.empty_cache()

        epoch_loss = running_loss / total_samples
        epoch_accuracy = correct_predictions / total_samples

        # Giai ƒëo·∫°n Validation
        model.eval() # ƒê·∫∑t m√¥ h√¨nh v·ªÅ ch·∫ø ƒë·ªô eval cho validation
        val_correct_predictions = 0
        val_total_samples = 0
        val_loss = 0.0
        with torch.no_grad():
            for inputs, labels, _ in val_loader: # Kh√¥ng c·∫ßn global_indices trong val_loader cho b∆∞·ªõc validation
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                val_loss += criterion(outputs, labels).item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                val_total_samples += labels.size(0)
                val_correct_predictions += (predicted == labels).sum().item()

        val_accuracy = val_correct_predictions / val_total_samples
        val_loss /= val_total_samples

        print(f"Epoch {epoch+1}: Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}, "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}")

        scheduler.step()

        # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n ƒë·ªô ch√≠nh x√°c validation
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), os.path.join(cfg.MODEL_SAVE_DIR, f'best_model_ensemble_{model_idx}.pth'))
            print(f"ƒê√£ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t {model_idx + 1} v·ªõi Val Acc: {best_val_accuracy:.4f}")

    print(f"Ho√†n th√†nh hu·∫•n luy·ªán M√¥ h√¨nh {model_idx + 1}. Val Acc t·ªët nh·∫•t: {best_val_accuracy:.4f}")
    
    # Sau t·∫•t c·∫£ c√°c epoch cho m√¥ h√¨nh n√†y, t√≠nh to√°n 'learning metrics' cho m·ªói m·∫´u
    sample_learning_metrics = {}
    for global_idx, history in training_prediction_details.items():
        correct_epochs_history = [h for h in history if h['is_correct']]
        
        avg_correct_confidence = np.mean([h['confidence'] for h in correct_epochs_history]) if correct_epochs_history else 0.0
        
        # S·ª± ch·∫≠m tr·ªÖ trong h·ªçc t·∫≠p: epoch ƒë·∫ßu ti√™n m√† n√≥ ƒë√∫ng v√† v·∫´n ƒë√∫ng cho t·∫•t c·∫£ c√°c epoch ti·∫øp theo
        first_correct_epoch = epochs # M·∫∑c ƒë·ªãnh l√† 'ch∆∞a bao gi·ªù h·ªçc th·ª±c s·ª±' (max epochs)
        for k_idx in range(len(history)): 
            if history[k_idx]['is_correct']:
                # Ki·ªÉm tra xem n√≥ c√≥ gi·ªØ ƒë√∫ng cho ƒë·∫øn cu·ªëi kh√¥ng
                if all(h_sub['is_correct'] for h_sub in history[k_idx:]):
                    first_correct_epoch = history[k_idx]['epoch']
                    break
        
        # T√≠nh nh·∫•t qu√°n: t·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng tr√™n t·∫•t c·∫£ c√°c epoch cho m√¥ h√¨nh n√†y
        consistency = len(correct_epochs_history) / epochs if epochs > 0 else 0.0

        sample_learning_metrics[global_idx] = {
            'avg_correct_confidence': avg_correct_confidence,
            'first_correct_epoch': first_correct_epoch,
            'consistency': consistency
        }
    
    return sample_learning_metrics

# --- 3. H√†m hu·∫•n luy·ªán cho c√°c th√†nh vi√™n Ensemble v·ªõi theo d√µi ƒë·ªông l·ª±c hu·∫•n luy·ªán ---
def train_model(model, train_loader, val_loader, epochs, lr, device, model_idx, cfg):
    """
    Hu·∫•n luy·ªán m·ªôt th·ªÉ hi·ªán duy nh·∫•t c·ªßa b·ªô ph√¢n lo·∫°i c∆° b·∫£n v√† theo d√µi ƒë·ªông l·ª±c hu·∫•n luy·ªán chi ti·∫øt
    (nh√£n d·ª± ƒëo√°n v√† ƒë·ªô tin c·∫≠y cho m·ªói m·∫´u ·ªü m·ªói epoch).
    """
    if cfg.LABEL_SMOOTHING_ENABLE:
        criterion = LabelSmoothingLoss(classes=2, epsilon=cfg.LABEL_SMOOTHING_EPSILON).to(device)
        print(f"ƒê√£ b·∫≠t Label Smoothing v·ªõi epsilon: {cfg.LABEL_SMOOTHING_EPSILON}")
    else:
        criterion = nn.CrossEntropyLoss().to(device)

    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)

    model.to(device)
    best_val_accuracy = 0.0
    
    # T·ª´ ƒëi·ªÉn ƒë·ªÉ l∆∞u tr·ªØ l·ªãch s·ª≠ d·ª± ƒëo√°n chi ti·∫øt cho m·ªói m·∫´u hu·∫•n luy·ªán qua c√°c epoch
    # Key: global_idx c·ªßa m·∫´u, Value: Danh s√°ch c√°c dict, m·ªói dict (epoch, is_correct, predicted_label, confidence)
    training_prediction_details = {global_idx: [] for global_idx in train_loader.dataset.global_indices}


    print(f"\n--- Hu·∫•n luy·ªán M√¥ h√¨nh Ensemble {model_idx + 1} ---")
    for epoch in range(epochs):
        model.train() # ƒê·∫£m b·∫£o m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô train (dropout ho·∫°t ƒë·ªông n·∫øu c√≥)
        running_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        for inputs, labels, global_indices_batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} (Train)", dynamic_ncols=True):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            
            # T√°ch c√°c outputs tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi sang numpy
            probs = softmax(outputs.detach().cpu().numpy(), axis=1) 
            predicted_labels = np.argmax(probs, axis=1)
            confidences_batch = np.max(probs, axis=1)

            total_samples += labels.size(0)
            correct_in_batch = (predicted_labels == labels.cpu().numpy()).sum().item()
            correct_predictions += correct_in_batch

            # C·∫≠p nh·∫≠t ƒë·ªông l·ª±c hu·∫•n luy·ªán: l∆∞u nh√£n d·ª± ƒëo√°n v√† ƒë·ªô tin c·∫≠y c·ªßa n√≥ cho m·ªói m·∫´u ·ªü epoch n√†y
            for i, global_idx_tensor in enumerate(global_indices_batch):
                global_idx = global_idx_tensor.item()
                is_correct_prediction = (predicted_labels[i] == labels[i].item())
                training_prediction_details[global_idx].append({
                    'epoch': epoch,
                    'is_correct': is_correct_prediction,
                    'predicted_label': predicted_labels[i],
                    'confidence': confidences_batch[i]
                })
        
        # Clear CUDA cache after each epoch to free up memory
        if device.type == 'cuda':
            torch.cuda.empty_cache()

        epoch_loss = running_loss / total_samples
        epoch_accuracy = correct_predictions / total_samples

        # Giai ƒëo·∫°n Validation
        model.eval() # ƒê·∫∑t m√¥ h√¨nh v·ªÅ ch·∫ø ƒë·ªô eval cho validation
        val_correct_predictions = 0
        val_total_samples = 0
        val_loss = 0.0
        with torch.no_grad():
            for inputs, labels, _ in val_loader: # Kh√¥ng c·∫ßn global_indices trong val_loader cho b∆∞·ªõc validation
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                val_loss += criterion(outputs, labels).item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                val_total_samples += labels.size(0)
                val_correct_predictions += (predicted == labels).sum().item()

        val_accuracy = val_correct_predictions / val_total_samples
        val_loss /= val_total_samples

        print(f"Epoch {epoch+1}: Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}, "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}")

        scheduler.step()

        # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n ƒë·ªô ch√≠nh x√°c validation
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), os.path.join(cfg.MODEL_SAVE_DIR, f'best_model_ensemble_{model_idx}.pth'))
            print(f"ƒê√£ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t {model_idx + 1} v·ªõi Val Acc: {best_val_accuracy:.4f}")

    print(f"Ho√†n th√†nh hu·∫•n luy·ªán M√¥ h√¨nh {model_idx + 1}. Val Acc t·ªët nh·∫•t: {best_val_accuracy:.4f}")
    
    # Sau t·∫•t c·∫£ c√°c epoch cho m√¥ h√¨nh n√†y, t√≠nh to√°n 'learning metrics' cho m·ªói m·∫´u
    sample_learning_metrics = {}
    for global_idx, history in training_prediction_details.items():
        correct_epochs_history = [h for h in history if h['is_correct']]
        
        avg_correct_confidence = np.mean([h['confidence'] for h in correct_epochs_history]) if correct_epochs_history else 0.0
        
        # S·ª± ch·∫≠m tr·ªÖ trong h·ªçc t·∫≠p: epoch ƒë·∫ßu ti√™n m√† n√≥ ƒë√∫ng v√† v·∫´n ƒë√∫ng cho t·∫•t c·∫£ c√°c epoch ti·∫øp theo
        first_correct_epoch = epochs # M·∫∑c ƒë·ªãnh l√† 'ch∆∞a bao gi·ªù h·ªçc th·ª±c s·ª±' (max epochs)
        for k_idx in range(len(history)): 
            if history[k_idx]['is_correct']:
                # Ki·ªÉm tra xem n√≥ c√≥ gi·ªØ ƒë√∫ng cho ƒë·∫øn cu·ªëi kh√¥ng
                if all(h_sub['is_correct'] for h_sub in history[k_idx:]):
                    first_correct_epoch = history[k_idx]['epoch']
                    break
        
        # T√≠nh nh·∫•t qu√°n: t·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng tr√™n t·∫•t c·∫£ c√°c epoch cho m√¥ h√¨nh n√†y
        consistency = len(correct_epochs_history) / epochs if epochs > 0 else 0.0

        sample_learning_metrics[global_idx] = {
            'avg_correct_confidence': avg_correct_confidence,
            'first_correct_epoch': first_correct_epoch,
            'consistency': consistency
        }
    
    return sample_learning_metrics

# Complete function for training whole ensemble

def train_ensemble(cfg, train_loader, val_loader):
    """
    Train ensemble c·ªßa NUM_ENSEMBLE_MODELS models v√† tr·∫£ v·ªÅ training dynamics.
    """
    print(f"üî• Training ensemble of {cfg.NUM_ENSEMBLE_MODELS} models...")
    
    # Kh·ªüi t·∫°o dictionary ƒë·ªÉ l∆∞u tr·ªØ learning metrics t·ª´ t·∫•t c·∫£ models
    overall_learning_metrics = {}
    
    for i in range(cfg.NUM_ENSEMBLE_MODELS):
        print(f"\nü§ñ Training model {i+1}/{cfg.NUM_ENSEMBLE_MODELS}")
        
        # Set different seed for diversity
        set_seed(cfg.RANDOM_SEED + i)
        
        # T·∫°o model m·ªõi cho m·ªói ensemble member
        model = BaseClassifier(
            num_classes=2, 
            dropout_rate=(cfg.MCDO_DROPOUT_RATE if cfg.MCDO_ENABLE else 0.0)
        )
        
        # Train model v√† thu th·∫≠p learning metrics
        individual_learning_metrics = train_model(
            model, train_loader, val_loader, 
            cfg.NUM_EPOCHS_PER_MODEL, cfg.LEARNING_RATE, 
            cfg.DEVICE, i, cfg
        )
        
        # Merge learning metrics t·ª´ model n√†y v√†o overall metrics  
        for global_idx, metrics in individual_learning_metrics.items():
            if global_idx not in overall_learning_metrics:
                overall_learning_metrics[global_idx] = {
                    'avg_correct_confidence_list': [],
                    'first_correct_epoch_list': [],
                    'consistency_list': []
                }
            
            overall_learning_metrics[global_idx]['avg_correct_confidence_list'].append(metrics['avg_correct_confidence'])
            overall_learning_metrics[global_idx]['first_correct_epoch_list'].append(metrics['first_correct_epoch'])
            overall_learning_metrics[global_idx]['consistency_list'].append(metrics['consistency'])
        
        # Clear CUDA cache after each model's training
        if cfg.DEVICE.type == 'cuda':
            torch.cuda.empty_cache()
    
    # Aggregate learning metrics across all ensemble members
    final_overall_learning_metrics = {}
    for global_idx, all_model_metrics in overall_learning_metrics.items():
        # T√≠nh average learning metrics across ensemble members v·ªõi t√™n ƒë√∫ng
        avg_conf_list = all_model_metrics['avg_correct_confidence_list']
        epoch_list = all_model_metrics['first_correct_epoch_list']
        consistency_list = all_model_metrics['consistency_list']
        
        final_overall_learning_metrics[global_idx] = {
            'mean_avg_correct_confidence': np.mean(avg_conf_list) if avg_conf_list else 0.0,
            'mean_first_correct_epoch': np.mean(epoch_list) if epoch_list else cfg.NUM_EPOCHS_PER_MODEL,
            'mean_consistency': np.mean(consistency_list) if consistency_list else 0.0
        }
    
    print(f"‚úÖ Ensemble training completed!")
    print(f"üìä Training dynamics tracked for {len(final_overall_learning_metrics)} samples")
    
    return final_overall_learning_metrics

# --- 5. ∆Ø·ªõc t√≠nh v√† hi·ªáu ch·ªânh ƒë·ªô tin c·∫≠y n√¢ng cao ---
class TemperatureScaler(nn.Module):
    """
    H·ªçc m·ªôt tham s·ªë nhi·ªát ƒë·ªô scalar duy nh·∫•t ƒë·ªÉ hi·ªáu ch·ªânh c√°c x√°c su·∫•t.
    D·ª±a tr√™n Guo et al. "On Calibration of Modern Neural Networks" (ICML 2017).
    """
    def __init__(self):
        super().__init__()
        self.temperature = nn.Parameter(torch.ones(1))

    def forward(self, logits):
        return logits / self.temperature

    def calibrate(self, logits_to_calibrate, labels_for_calibration, device):
        """
        ƒêi·ªÅu ch·ªânh tham s·ªë nhi·ªát ƒë·ªô b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c logits v√† nh√£n ƒë√£ t√≠nh to√°n tr∆∞·ªõc.
        """
        # ƒê·∫£m b·∫£o logits v√† nh√£n n·∫±m tr√™n thi·∫øt b·ªã ch√≠nh x√°c
        logits_all = logits_to_calibrate.to(device)
        labels_all = labels_for_calibration.to(device)

        nll_criterion = nn.CrossEntropyLoss().to(device)
        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50, line_search_fn='strong_wolfe')

        def eval():
            optimizer.zero_grad()
            loss = nll_criterion(self.forward(logits_all), labels_all)
            loss.backward()
            return loss

        optimizer.step(eval)
        print(f"B·ªô hi·ªáu ch·ªânh nhi·ªát ƒë·ªô ƒë√£ ƒë∆∞·ª£c hi·ªáu ch·ªânh. T·ªëi ∆∞u T: {self.temperature.item():.4f}")

class IsotonicCalibrator:
    """
    Hi·ªáu ch·ªânh b·∫±ng Isotonic Regression.
    """
    def __init__(self):
        self.ir = IsotonicRegression(out_of_bounds="clip")

    def calibrate(self, confidences_to_calibrate, labels_for_calibration):
        # confidences_to_calibrate should be 1D array of confidence scores
        # labels_for_calibration should be 1D array of binary labels (0 or 1)
        self.ir.fit(confidences_to_calibrate, labels_for_calibration)
        print("Isotonic Regression ƒë√£ ƒë∆∞·ª£c hi·ªáu ch·ªânh.")

    def predict_proba(self, confidences_to_transform):
        return self.ir.transform(confidences_to_transform)


class BetaCalibrator:
    """
    Hi·ªáu ch·ªânh b·∫±ng Beta Calibration.
    Tham s·ªë alpha v√† beta c·ªßa ph√¢n ph·ªëi Beta ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a.
    """
    def __init__(self):
        self.alpha = None
        self.beta = None
    
    def _objective_function(self, params, confidences, labels):
        alpha, beta = params
        # Clip confidences to avoid log(0) or log(1) issues
        conf_clamped = np.clip(confidences, 1e-10, 1 - 1e-10)
        
        # Apply transformation: logit(p_calibrated) = sigmoid(alpha * logit(p_original) + beta)
        logit_original = np.log(conf_clamped / (1 - conf_clamped))
        calibrated_confidences = 1.0 / (1.0 + np.exp(- (alpha * logit_original + beta)))
        
        # Clamp again to avoid log(0) or log(1)
        calibrated_confidences = np.clip(calibrated_confidences, 1e-10, 1 - 1e-10)
        
        # Negative Log-Likelihood as objective
        nll = -np.mean(labels * np.log(calibrated_confidences) + (1 - labels) * np.log(1 - calibrated_confidences))
        return nll

    def calibrate(self, confidences_to_calibrate, labels_for_calibration):
        # Initial guess for alpha and beta
        initial_params = [1.0, 0.0] # alpha=1.0, beta=0.0 means no change (identity)
        
        # Perform optimization using L-BFGS-B (bounded to avoid extreme values)
        result = minimize(self._objective_function, initial_params, 
                          args=(confidences_to_calibrate, labels_for_calibration), 
                          method='L-BFGS-B', 
                          bounds=[(0.01, None), (None, None)]) # alpha must be positive
        
        self.alpha, self.beta = result.x
        print(f"Beta Calibration ƒë√£ ƒë∆∞·ª£c hi·ªáu ch·ªânh. Alpha: {self.alpha:.4f}, Beta: {self.beta:.4f}")

    def predict_proba(self, confidences_to_transform):
        if self.alpha is None or self.beta is None:
            raise ValueError("BetaCalibrator ch∆∞a ƒë∆∞·ª£c hi·ªáu ch·ªânh. Vui l√≤ng g·ªçi .calibrate() tr∆∞·ªõc.")
        
        conf_clamped = np.clip(confidences_to_transform, 1e-10, 1 - 1e-10)
        logit_original = np.log(conf_clamped / (1 - conf_clamped))
        calibrated_conf = 1.0 / (1.0 + np.exp(- (self.alpha * logit_original + self.beta)))
        return np.clip(calibrated_conf, 0.0, 1.0)

def extract_features(model, data_loader, device):
    """
    Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ feature_extractor c·ªßa m√¥ h√¨nh cho t·∫•t c·∫£ c√°c m·∫´u trong data_loader.
    Tr·∫£ v·ªÅ c√°c ƒë·∫∑c tr∆∞ng d∆∞·ªõi d·∫°ng m·∫£ng numpy v√† c√°c ch·ªâ m·ª•c to√†n c·ª•c g·ªëc t∆∞∆°ng ·ª©ng.
    """
    model.eval() # ƒê·∫£m b·∫£o model ·ªü ch·∫ø ƒë·ªô eval cho vi·ªác tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng th√¥ng th∆∞·ªùng
    all_features = []
    all_indices = []
    with torch.no_grad():
        for inputs, _, global_indices_batch in tqdm(data_loader, desc="Tr√≠ch xu·∫•t ƒê·∫∑c tr∆∞ng", dynamic_ncols=True):
            inputs = inputs.to(device)
            features = model.get_features(inputs) # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c get_features m·ªõi
            all_features.append(features.cpu().numpy())
            all_indices.extend(global_indices_batch.cpu().numpy())
    return np.vstack(all_features), np.array(all_indices)


def adjust_confidence_with_training_dynamics(cfg, test_features, current_scores,
                                             train_features, train_global_indices, final_overall_learning_metrics):
    """
    ƒêi·ªÅu ch·ªânh ƒëi·ªÉm tin c·∫≠y/t·ª´ ch·ªëi c·ªßa t·∫≠p test d·ª±a tr√™n s·ª± t∆∞∆°ng ƒë·ªìng v·ªõi c√°c m·∫´u hu·∫•n luy·ªán v√† ƒë·ªông l·ª±c h·ªçc c·ªßa ch√∫ng.
    ƒêi·ªÉm th·∫•p h∆°n ƒë·ªëi v·ªõi c√°c m·∫´u test t∆∞∆°ng t·ª± c√°c m·∫´u hu·∫•n luy·ªán 'kh√≥' (v√≠ d·ª•: h·ªçc mu·ªôn, kh√¥ng nh·∫•t qu√°n).
    """
    print("ƒêi·ªÅu ch·ªânh ƒëi·ªÉm c·ªßa t·∫≠p test v·ªõi ƒë·ªông l·ª±c hu·∫•n luy·ªán...")
    adjusted_scores = np.copy(current_scores)

    # T·∫°o √°nh x·∫° t·ª´ global_idx ƒë·∫øn t·ª´ ƒëi·ªÉn learning metrics ƒë·ªÉ tra c·ª©u hi·ªáu qu·∫£
    train_global_idx_to_metrics = {idx: metrics for idx, metrics in final_overall_learning_metrics.items()}

    # T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa c√°c ƒë·∫∑c tr∆∞ng test v√† hu·∫•n luy·ªán
    if len(test_features) == 0 or len(train_features) == 0:
        print("B·ªè qua ƒëi·ªÅu ch·ªânh ƒë·ªông l·ª±c hu·∫•n luy·ªán: Kh√¥ng c√≥ ƒë·∫∑c tr∆∞ng test ho·∫∑c hu·∫•n luy·ªán.")
        return adjusted_scores

    similarities = cosine_similarity(test_features, train_features)
    
    for i in tqdm(range(len(test_features)), desc="√Åp d·ª•ng ƒëi·ªÅu ch·ªânh ƒë·ªông l·ª±c hu·∫•n luy·ªán", dynamic_ncols=True):
        # T√¨m m·∫´u hu·∫•n luy·ªán t∆∞∆°ng ƒë·ªìng nh·∫•t (b·∫±ng ch·ªâ m·ª•c c·ªßa n√≥ trong m·∫£ng `train_features`)
        most_similar_train_idx_in_features_array = np.argmax(similarities[i])
        # L·∫•y ch·ªâ m·ª•c to√†n c·ª•c g·ªëc c·ªßa m·∫´u hu·∫•n luy·ªán t∆∞∆°ng ƒë·ªìng nh·∫•t ƒë√≥
        most_similar_train_global_idx = train_global_indices[most_similar_train_idx_in_features_array]
        
        # Ki·ªÉm tra xem c√°c learning metrics cho ch·ªâ m·ª•c to√†n c·ª•c n√†y c√≥ t·ªìn t·∫°i kh√¥ng
        if most_similar_train_global_idx in train_global_idx_to_metrics:
            sample_metrics = train_global_idx_to_metrics[most_similar_train_global_idx]
            
            # S·ª≠ d·ª•ng 'mean_first_correct_epoch' l√†m proxy cho 'learning lateness' ho·∫∑c 'difficulty'.
            # 'mean_first_epoch' cao h∆°n cho th·∫•y m·ªôt m·∫´u kh√≥ h·ªçc h∆°n.
            # S·ª≠ d·ª•ng .get() v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh trong tr∆∞·ªùng h·ª£p key b·ªã thi·∫øu b·∫•t ng·ªù
            difficulty_value = sample_metrics.get('mean_first_correct_epoch', cfg.NUM_EPOCHS_PER_MODEL)
            
            # Chu·∫©n h√≥a ƒë·ªô kh√≥ n·∫±m gi·ªØa 0 v√† 1 (0 = d·ªÖ, 1 = kh√≥).
            # N·∫øu m·ªôt m·∫´u ƒë∆∞·ª£c h·ªçc mu·ªôn (epoch cao h∆°n), n√≥ kh√≥ h∆°n, v√¨ v·∫≠y ƒë·ªô kh√≥ chu·∫©n h√≥a g·∫ßn 1.
            if cfg.NUM_EPOCHS_PER_MODEL > 0:
                normalized_difficulty = difficulty_value / cfg.NUM_EPOCHS_PER_MODEL
            else:
                normalized_difficulty = 0.0 # M·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ epoch n√†o ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a (ho·∫∑c 0.5 cho trung t√≠nh)


        else:
            # N·∫øu kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c (v√≠ d·ª•: m·ªôt m·∫´u hu·∫•n luy·ªán b·∫±ng c√°ch n√†o ƒë√≥ b·ªã b·ªè l·ª° trong qu√° tr√¨nh theo d√µi),
            # m·∫∑c ƒë·ªãnh kh√¥ng c√≥ h√¨nh ph·∫°t (ƒë·ªô kh√≥ trung t√≠nh).
            normalized_difficulty = 0.0 

        # Gi·∫£m ƒëi·ªÉm d·ª±a tr√™n ƒë·ªô kh√≥ v√† m·ªôt y·∫øu t·ªë h√¨nh ph·∫°t c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh (cfg.TRAINING_DYNAMICS_CONF_PENALTY)
        # ƒê·ªô kh√≥ cao h∆°n d·∫´n ƒë·∫øn gi·∫£m ƒëi·ªÉm l·ªõn h∆°n
        adjustment_factor = 1.0 - (cfg.TRAINING_DYNAMICS_CONF_PENALTY * normalized_difficulty)
        
        adjusted_scores[i] *= adjustment_factor
        adjusted_scores[i] = max(0.0, adjusted_scores[i]) # ƒê·∫£m b·∫£o ƒëi·ªÉm kh√¥ng √¢m

    return adjusted_scores
def calculate_single_model_odin_score(model, inputs, temp, epsilon, device):
    """
    T√≠nh to√°n ƒëi·ªÉm ODIN cho ƒë·∫ßu v√†o batch tr√™n m·ªôt m√¥ h√¨nh duy nh·∫•t.
    Tr·∫£ v·ªÅ ƒëi·ªÉm ODIN (numpy array), ƒëi·ªÉm cao h∆°n nghƒ©a l√† trong ph√¢n ph·ªëi h∆°n.
    """
    # ƒê·∫£m b·∫£o inputs c√≥ th·ªÉ t√≠nh to√°n gradient
    inputs.requires_grad_(True)
    
    # ƒê·∫∑t m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô ƒë√°nh gi√°
    model.eval() 
    
    # Forward pass ƒë·ªÉ l·∫•y logits
    outputs = model(inputs)
    
    # √Åp d·ª•ng nhi·ªát ƒë·ªô cho logits
    temp_outputs = outputs / temp
    
    # L·∫•y l·ªõp d·ª± ƒëo√°n cho vi·ªác nhi·ªÖu lo·∫°n
    pred_class = temp_outputs.argmax(dim=1)
    
    # T√≠nh to√°n loss (negative log-likelihood) cho l·ªõp d·ª± ƒëo√°n.
    # M·ª•c ti√™u l√† t·ªëi ƒëa h√≥a x√°c su·∫•t c·ªßa l·ªõp d·ª± ƒëo√°n n√†y b·∫±ng c√°ch nhi·ªÖu lo·∫°n ƒë·∫ßu v√†o.
    loss = F.cross_entropy(temp_outputs, pred_class)
    
    # T√≠nh to√°n gradient c·ªßa loss ƒë·ªëi v·ªõi ƒë·∫ßu v√†o
    # create_graph=False ƒë·ªÉ kh√¥ng x√¢y d·ª±ng bi·ªÉu ƒë·ªì cho c√°c l·∫ßn backward ti·∫øp theo
    grad = torch.autograd.grad(loss, inputs, create_graph=False)[0] 
    
    # T·∫°o ƒë·∫ßu v√†o b·ªã nhi·ªÖu lo·∫°n
    perturbed_inputs = inputs - epsilon * torch.sign(-grad)
    
    # Chuy·ªÉn ƒë·∫ßu v√†o b·ªã nhi·ªÖu lo·∫°n qua m√¥ h√¨nh m·ªôt l·∫ßn n·ªØa
    with torch.no_grad(): # Kh√¥ng c·∫ßn gradient cho b∆∞·ªõc n√†y
        perturbed_outputs = model(perturbed_inputs)
        
    # ƒêi·ªÉm ODIN l√† x√°c su·∫•t t·ªëi ƒëa c·ªßa ƒë·∫ßu ra b·ªã nhi·ªÖu lo·∫°n sau khi √°p d·ª•ng nhi·ªát ƒë·ªô
    odin_probs = F.softmax(perturbed_outputs / temp, dim=1)
    odin_score = torch.max(odin_probs, dim=1)[0]
    
    inputs.requires_grad_(False) # ƒê·∫∑t l·∫°i y√™u c·∫ßu gradient c·ªßa ƒë·∫ßu v√†o
    
    return odin_score.cpu().numpy()
def get_rejection_scores_and_predictions(cfg, data_loader, ensemble_models_dir, 
                                        final_overall_learning_metrics=None, train_dataset=None,
                                        calibration_method='temperature_scaling', # e.g., 'temperature_scaling', 'isotonic_regression', 'beta_calibration'
                                        ood_detection_method='none', # e.g., 'none', 'odin', 'energy'
                                        combine_ood_with_disagreement=False): # Controls B.1.2, B.2.2 vs B.1.1, B.2.1
    """
    T√≠nh to√°n ƒëi·ªÉm t·ª´ ch·ªëi v√† d·ª± ƒëo√°n c·ªßa ensemble cho c√°c m·∫´u.
    T√πy ch·ªçn t√≠ch h·ª£p Monte Carlo Dropout (MCDO), c√°c ph∆∞∆°ng ph√°p hi·ªáu ch·ªânh,
    v√† c√°c ph∆∞∆°ng ph√°p ph√°t hi·ªán OOD (ODIN, Energy Score).
    √Åp d·ª•ng ƒëi·ªÅu ch·ªânh d·ª±a tr√™n ƒë·ªông l·ª±c hu·∫•n luy·ªán n·∫øu `cfg.ENABLE_TRAINING_DYNAMICS` l√† True.

    Tr·∫£ v·ªÅ:
    - all_predictions: C√°c d·ª± ƒëo√°n ensemble cu·ªëi c√πng
    - final_rejection_scores: ƒêi·ªÉm t·ª´ ch·ªëi cu·ªëi c√πng (cao h∆°n l√† ƒë∆∞·ª£c ch·∫•p nh·∫≠n)
    - all_labels: Nh√£n th·ª±c
    - all_original_indices: C√°c ch·ªâ m·ª•c to√†n c·ª•c g·ªëc c·ªßa c√°c m·∫´u
    - all_ensemble_individual_probs_stacked: M·∫£ng NumPy (num_samples, total_runs_or_models, num_classes) c·ªßa c√°c x√°c su·∫•t m√¥ h√¨nh ri√™ng l·∫ª/ch·∫°y MCDO.
    - all_odin_scores_raw: ƒêi·ªÉm ODIN th√¥ cho m·ªói m·∫´u (None n·∫øu kh√¥ng √°p d·ª•ng)
    - all_energy_scores_raw: ƒêi·ªÉm Energy th√¥ cho m·ªói m·∫´u (None n·∫øu kh√¥ng √°p d·ª•ng)
    """
    all_predictions = []
    all_labels = []
    all_original_indices = []
    
    all_calibrated_confidences = [] # ƒêi·ªÉm tin c·∫≠y sau hi·ªáu ch·ªânh, tr∆∞·ªõc b·∫•t ƒë·ªìng/OOD
    
    all_individual_run_probs_across_batches_if_mcdo_enabled = [] # ƒê·ªÉ t√≠nh to√°n b·∫•t ƒë·ªìng
    all_odin_scores_across_batches = [] # ƒêi·ªÉm ODIN trung b√¨nh cho m·ªói m·∫´u
    all_energy_scores_across_batches = [] # ƒêi·ªÉm Energy cho m·ªói m·∫´u

    # T·∫£i t·∫•t c·∫£ c√°c m√¥ h√¨nh ensemble
    loaded_models = []
    for i in range(cfg.NUM_ENSEMBLE_MODELS):
        model = BaseClassifier(num_classes=2, dropout_rate=(cfg.MCDO_DROPOUT_RATE if cfg.MCDO_ENABLE else 0.0)).to(cfg.DEVICE)
        model.load_state_dict(torch.load(os.path.join(ensemble_models_dir, f'best_model_ensemble_{i}.pth')))
        model.eval() # Lu√¥n ·ªü ch·∫ø ƒë·ªô eval cho inference khi kh√¥ng d√πng MCDO, nh∆∞ng s·∫Ω b·∫≠t dropout n·∫øu MCDO_ENABLE
        loaded_models.append(model)
    
    # Clear CUDA cache before starting inference/calibration to ensure maximum free memory
    if cfg.DEVICE.type == 'cuda':
        torch.cuda.empty_cache()
            # Kh·ªüi t·∫°o b·ªô hi·ªáu ch·ªânh
    calibrator = None
    if calibration_method == 'temperature_scaling':
        calibrator = TemperatureScaler()
        calibrator.to(cfg.DEVICE)
        print("S·ª≠ d·ª•ng Temperature Scaling ƒë·ªÉ hi·ªáu ch·ªânh.")
    elif calibration_method == 'isotonic_regression':
        calibrator = IsotonicCalibrator()
        print("S·ª≠ d·ª•ng Isotonic Regression ƒë·ªÉ hi·ªáu ch·ªânh.")
    elif calibration_method == 'beta_calibration':
        calibrator = BetaCalibrator()
        print("S·ª≠ d·ª•ng Beta Calibration ƒë·ªÉ hi·ªáu ch·ªânh.")
    else:
        print("Kh√¥ng s·ª≠ d·ª•ng hi·ªáu ch·ªânh post-hoc (ho·∫∑c ph∆∞∆°ng ph√°p kh√¥ng h·ª£p l·ªá).")

    print(f"Hi·ªáu ch·ªânh b·ªô hi·ªáu ch·ªânh ({calibration_method}) tr√™n c√°c logits/x√°c su·∫•t trung b√¨nh c·ªßa ensemble t·ª´ data_loader hi·ªán t·∫°i...")
    # --- Thu th·∫≠p t·∫•t c·∫£ c√°c logits/confidences/labels t·ª´ data_loader hi·ªán t·∫°i ƒë·ªÉ hi·ªáu ch·ªânh ---
    data_loader_ensemble_raw_outputs = [] # Logits ho·∫∑c x√°c su·∫•t trung b√¨nh
    data_loader_labels_for_calibration = []

    with torch.no_grad():
        for inputs_batch_cal, labels_batch_cal, _ in tqdm(data_loader, desc="Thu th·∫≠p D·ªØ li·ªáu ƒë·ªÉ hi·ªáu ch·ªânh", dynamic_ncols=True):
            inputs_batch_cal = inputs_batch_cal.to(cfg.DEVICE)
            
            ensemble_logits_batch_cal = []
            if cfg.MCDO_ENABLE:
                for model_idx, model in enumerate(loaded_models):
                    model.train() # Enable dropout for MCDO during inference for avg_logits for calibration
                    model_logits_runs = []
                    for _ in range(cfg.MCDO_NUM_RUNS):
                        model_logits_runs.append(model(inputs_batch_cal))
                    ensemble_logits_batch_cal.append(torch.stack(model_logits_runs).mean(dim=0))
            else:
                for model_idx, model in enumerate(loaded_models):
                    model.eval()
                    ensemble_logits_batch_cal.append(model(inputs_batch_cal))
            
            avg_logits_batch_cal = torch.stack(ensemble_logits_batch_cal).mean(dim=0)
            
            data_loader_ensemble_raw_outputs.append(avg_logits_batch_cal.cpu()) # Move to CPU
            data_loader_labels_for_calibration.append(labels_batch_cal.cpu())

    # Clear CUDA cache after collecting data for calibration
    if cfg.DEVICE.type == 'cuda':
        torch.cuda.empty_cache()

    if data_loader_ensemble_raw_outputs: 
        data_loader_ensemble_raw_outputs_all = torch.cat(data_loader_ensemble_raw_outputs)
        data_loader_labels_for_calibration_all = torch.cat(data_loader_labels_for_calibration)

        if calibration_method == 'temperature_scaling':
            calibrator.calibrate(data_loader_ensemble_raw_outputs_all.to(cfg.DEVICE), # Move back to GPU for calibration
                                 data_loader_labels_for_calibration_all.to(cfg.DEVICE), cfg.DEVICE)
        elif calibration_method in ['isotonic_regression', 'beta_calibration']:
            probs_for_calibration = softmax(data_loader_ensemble_raw_outputs_all.numpy(), axis=1)
            confidences_for_calibration = np.max(probs_for_calibration, axis=1)
            calibrator.calibrate(confidences_for_calibration, data_loader_labels_for_calibration_all.numpy())
    else:
        print("C·∫£nh b√°o: Kh√¥ng c√≥ d·ªØ li·ªáu trong data_loader ƒë·ªÉ hi·ªáu ch·ªânh. B·ªè qua hi·ªáu ch·ªânh post-hoc.")

    # Clear CUDA cache after calibration
    if cfg.DEVICE.type == 'cuda':
        torch.cuda.empty_cache()
    
    # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ data_loader hi·ªán t·∫°i (t·∫≠p val/test) ƒë·ªÉ ƒëi·ªÅu ch·ªânh ƒë·ªông l·ª±c hu·∫•n luy·ªán
    print("Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ tr√¨nh t·∫£i d·ªØ li·ªáu hi·ªán t·∫°i ƒë·ªÉ ƒëi·ªÅu ch·ªânh ƒë·ªô tin c·∫≠y...")
    loaded_models[0].eval() # ƒê·∫£m b·∫£o m√¥ h√¨nh ·ªü ch·∫ø ƒë·ªô eval khi tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
    all_extracted_features, extracted_original_indices = extract_features(
        loaded_models[0], data_loader, cfg.DEVICE 
    )

    # Clear CUDA cache after feature extraction
    if cfg.DEVICE.type == 'cuda':
        torch.cuda.empty_cache()

    print("T·∫°o d·ª± ƒëo√°n v√† c√°c ƒëi·ªÉm t·ª´ ch·ªëi...")
    for inputs, labels, original_indices_batch in tqdm(data_loader, desc="D·ª± ƒëo√°n v√† T√≠nh ƒëi·ªÉm", dynamic_ncols=True):
        inputs, labels = inputs.to(cfg.DEVICE), labels.to(cfg.DEVICE)

        ensemble_logits_per_model = [] 
        current_batch_ensemble_run_probs = [] # For all_ensemble_individual_probs_stacked

        # Calculate ODIN/Energy for each sample/model
        current_batch_odin_scores_individual_models = []
        current_batch_energy_scores_individual_models = []

        for model_idx, model in enumerate(loaded_models):
            # Ensure model is in correct mode (train for MCDO, eval otherwise)
            if cfg.MCDO_ENABLE:
                model.train() # Enable dropout during inference for MCDO
            else:
                model.eval()

            with torch.no_grad(): # Use no_grad for main inference
                if cfg.MCDO_ENABLE:
                    model_logits_runs = []
                    for _ in range(cfg.MCDO_NUM_RUNS):
                        model_logits_runs.append(model(inputs).cpu()) # Move logits to CPU immediately
                    
                    avg_logits_for_model = torch.stack(model_logits_runs).mean(dim=0).to(cfg.DEVICE)
                    ensemble_logits_per_model.append(avg_logits_for_model)

                    # Store all MCDO runs' probabilities for disagreement calculation
                    probs_runs = softmax(torch.stack([l.to(cfg.DEVICE) for l in model_logits_runs]).detach().cpu().numpy(), axis=2)
                    current_batch_ensemble_run_probs.append(np.transpose(probs_runs, (1, 0, 2)))
                else: # MCDO is disabled, just one forward pass per model
                    logits = model(inputs)
                    ensemble_logits_per_model.append(logits)
                    
                    # Store single run probabilities for disagreement calculation
                    probs_individual = softmax(logits.detach().cpu().numpy(), axis=1)
                    current_batch_ensemble_run_probs.append(probs_individual[:, np.newaxis, :])

            # --- Calculate ODIN Score for each model ---
            if ood_detection_method == 'odin':
                odin_score_batch = calculate_single_model_odin_score(model, inputs.clone().detach(), cfg.ODIN_TEMP, cfg.ODIN_EPSILON, cfg.DEVICE)
                current_batch_odin_scores_individual_models.append(odin_score_batch)
            
            # --- Calculate Energy Score for each model's logits ---
            if ood_detection_method == 'energy':
                with torch.no_grad():
                    if cfg.MCDO_ENABLE:
                        energy_score_batch = -torch.logsumexp(avg_logits_for_model, dim=1).detach().cpu().numpy()
                    else: # If MCDO is disabled, use the single pass logits
                        energy_score_batch = -torch.logsumexp(logits, dim=1).detach().cpu().numpy()
                current_batch_energy_scores_individual_models.append(energy_score_batch)

        # Clear CUDA cache after processing each model within the batch loop
        if cfg.DEVICE.type == 'cuda':
            torch.cuda.empty_cache()

        # Average ODIN/Energy scores across ensemble models for the current batch
        if current_batch_odin_scores_individual_models:
            all_odin_scores_across_batches.append(np.mean(np.stack(current_batch_odin_scores_individual_models, axis=1), axis=1))
        if current_batch_energy_scores_individual_models:
            all_energy_scores_across_batches.append(np.mean(np.stack(current_batch_energy_scores_individual_models, axis=1), axis=1))

        # Concatenate individual run probs for current batch across models
        if current_batch_ensemble_run_probs:
            all_individual_run_probs_across_batches_if_mcdo_enabled.append(np.concatenate(current_batch_ensemble_run_probs, axis=1))
        
        # Average logits from ensemble for this batch
        avg_ensemble_logits = torch.stack(ensemble_logits_per_model).mean(dim=0)

        # --- Apply Calibrator ---
        if calibrator:
            if calibration_method == 'temperature_scaling':
                calibrated_logits = calibrator.forward(avg_ensemble_logits)
                calibrated_probs = softmax(calibrated_logits.detach().cpu().numpy(), axis=1)
            elif calibration_method in ['isotonic_regression', 'beta_calibration']:
                initial_probs = softmax(avg_ensemble_logits.detach().cpu().numpy(), axis=1)
                initial_confidences = np.max(initial_probs, axis=1)
                
                calibrated_confidences_vals = calibrator.predict_proba(initial_confidences)
                calibrated_probs = initial_probs.copy()
                for k_idx in range(len(calibrated_probs)):
                    predicted_class = np.argmax(calibrated_probs[k_idx])
                    if calibrated_probs[k_idx][predicted_class] > 0:
                        scaling_factor = calibrated_confidences_vals[k_idx] / calibrated_probs[k_idx][predicted_class]
                        calibrated_probs[k_idx, :] *= scaling_factor
                        calibrated_probs[k_idx, :] = np.maximum(0, calibrated_probs[k_idx, :])
                        calibrated_probs[k_idx, :] /= (np.sum(calibrated_probs[k_idx, :]) + 1e-9)
                    else: # Fallback for edge case
                        calibrated_probs[k_idx, predicted_class] = calibrated_confidences_vals[k_idx]
                        other_class_indices = [j for j in range(calibrated_probs.shape[1]) if j != predicted_class]
                        if len(other_class_indices) > 0:
                            total_other_prob = 1.0 - calibrated_confidences_vals[k_idx]
                            if np.sum(calibrated_probs[k_idx, other_class_indices]) > 0:
                                calibrated_probs[k_idx, other_class_indices] *= (total_other_prob / np.sum(calibrated_probs[k_idx, other_class_indices]))
                            else:
                                calibrated_probs[k_idx, other_class_indices] = total_other_prob / len(other_class_indices)
        else: # No post-hoc calibration
            calibrated_probs = softmax(avg_ensemble_logits.detach().cpu().numpy(), axis=1)

        # This is the confidence after calibration, BEFORE disagreement/OOD penalty
        current_calibrated_confidences = np.max(calibrated_probs, axis=1)
        all_calibrated_confidences.extend(current_calibrated_confidences)

        # Store predictions and true labels
        predictions = np.argmax(calibrated_probs, axis=1)
        all_predictions.extend(predictions)
        all_labels.extend(labels.cpu().numpy())
        all_original_indices.extend(original_indices_batch.cpu().numpy())
    
    # --- After iterating through all batches, stack probabilities for disagreement ---
    if all_individual_run_probs_across_batches_if_mcdo_enabled:
        all_ensemble_individual_probs_stacked = np.concatenate(all_individual_run_probs_across_batches_if_mcdo_enabled, axis=0)
    else:
        all_ensemble_individual_probs_stacked = np.array([])

    # Consolidate ODIN and Energy scores
    all_odin_scores_raw = np.concatenate(all_odin_scores_across_batches, axis=0) if all_odin_scores_across_batches else None
    all_energy_scores_raw = np.concatenate(all_energy_scores_across_batches, axis=0) if all_energy_scores_across_batches else None

    # Convert lists to numpy arrays
    all_calibrated_confidences = np.array(all_calibrated_confidences)
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_original_indices = np.array(all_original_indices)

    # --- Calculate Disagreement Penalty ---
    disagreement_penalties = np.zeros_like(all_calibrated_confidences)
    if all_ensemble_individual_probs_stacked.size > 0:
        num_samples = all_ensemble_individual_probs_stacked.shape[0]
        for i_idx in range(num_samples):
            current_sample_individual_probs = all_ensemble_individual_probs_stacked[i_idx, :, :] 
            ensemble_predicted_class = all_predictions[i_idx] 
            
            if (current_sample_individual_probs.size > 0 and 
                ensemble_predicted_class < current_sample_individual_probs.shape[1] and 
                ensemble_predicted_class >= 0):
                variance_disagreement = np.var(current_sample_individual_probs[:, ensemble_predicted_class])
            else:
                variance_disagreement = 0.0 
            
            disagreement_penalties[i_idx] = variance_disagreement * cfg.DISAGREEMENT_PENALTY_FACTOR

    # --- Calculate Final Rejection Score based on Method ---
    final_rejection_scores = np.copy(all_calibrated_confidences)

    if ood_detection_method == 'none':
        final_rejection_scores = all_calibrated_confidences * (1.0 - disagreement_penalties)
        final_rejection_scores = np.clip(final_rejection_scores, 0.0, 1.0)
    elif ood_detection_method == 'odin':
        if all_odin_scores_raw is None or all_odin_scores_raw.size == 0:
            print("C·∫£nh b√°o: ODIN ƒë∆∞·ª£c y√™u c·∫ßu nh∆∞ng kh√¥ng c√≥ ƒëi·ªÉm ODIN. S·ª≠ d·ª•ng ƒëi·ªÉm tin c·∫≠y th√¥ng th∆∞·ªùng.")
            final_rejection_scores = all_calibrated_confidences * (1.0 - disagreement_penalties)
        else:
            final_rejection_scores = all_odin_scores_raw
            if combine_ood_with_disagreement:
                final_rejection_scores = final_rejection_scores * (1.0 - disagreement_penalties)
            final_rejection_scores = np.clip(final_rejection_scores, 0.0, 1.0)
    elif ood_detection_method == 'energy':
        if all_energy_scores_raw is None or all_energy_scores_raw.size == 0:
            print("C·∫£nh b√°o: Energy Score ƒë∆∞·ª£c y√™u c·∫ßu nh∆∞ng kh√¥ng c√≥ ƒëi·ªÉm Energy. S·ª≠ d·ª•ng ƒëi·ªÉm tin c·∫≠y th√¥ng th∆∞·ªùng.")
            final_rejection_scores = all_calibrated_confidences * (1.0 - disagreement_penalties)
        else:
            if all_energy_scores_raw.size > 0:
                min_e, max_e = np.min(all_energy_scores_raw), np.max(all_energy_scores_raw)
                if (max_e - min_e) > 0:
                    normalized_energy_scores = (all_energy_scores_raw - min_e) / (max_e - min_e)
                else: 
                    normalized_energy_scores = np.full_like(all_energy_scores_raw, 0.5) 
            else:
                normalized_energy_scores = np.array([])

            final_rejection_scores = normalized_energy_scores
            if combine_ood_with_disagreement:
                final_rejection_scores = final_rejection_scores * (1.0 - disagreement_penalties)
            final_rejection_scores = np.clip(final_rejection_scores, 0.0, 1.0)

    # --- √Åp d·ª•ng ƒêi·ªÅu ch·ªânh ƒê·ªông l·ª±c Hu·∫•n luy·ªán n·∫øu c·ªù ENABLE_TRAINING_DYNAMICS l√† True ---
    if cfg.ENABLE_TRAINING_DYNAMICS and final_overall_learning_metrics is not None and train_dataset is not None:
        # 'all_extracted_features' ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n ·ªü ƒë·∫ßu h√†m cho data_loader hi·ªán t·∫°i
        loaded_models[0].eval() # Ensure eval mode for feature extraction
        train_features_for_adjustment, train_global_indices_for_adjustment = extract_features(
            loaded_models[0], DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=2), cfg.DEVICE
        )

        final_rejection_scores = adjust_confidence_with_training_dynamics(
            cfg, all_extracted_features, final_rejection_scores, 
            train_features_for_adjustment, train_global_indices_for_adjustment, final_overall_learning_metrics
        )
    elif cfg.ENABLE_TRAINING_DYNAMICS:
        print("C·∫£nh b√°o: ENABLE_TRAINING_DYNAMICS b·∫≠t nh∆∞ng final_overall_learning_metrics ho·∫∑c train_dataset kh√¥ng ƒë∆∞·ª£c cung c·∫•p.")

    # Clear CUDA cache at the very end of the function
    if cfg.DEVICE.type == 'cuda':
        torch.cuda.empty_cache()

    return (all_predictions, final_rejection_scores, all_labels,
            all_original_indices, all_ensemble_individual_probs_stacked,
            all_odin_scores_raw, all_energy_scores_raw)


# --- ECE Calculation ---
def calculate_ece(model_predictions, rejection_scores, true_labels, num_bins=10):
    """
    T√≠nh to√°n Expected Calibration Error (ECE).
    """
    if len(rejection_scores) == 0:
        return 0.0

    bins = np.linspace(0., 1., num_bins + 1)
    ece = 0.0
    total_samples = len(true_labels)

    for i in range(num_bins):
        lower_bound = bins[i]
        upper_bound = bins[i+1]
        # FIX: Added np.nan_to_num to handle potential NaNs from previous calculations before comparison
        mask = (np.nan_to_num(rejection_scores, nan=-np.inf) >= lower_bound) & (np.nan_to_num(rejection_scores, nan=-np.inf) < upper_bound)
        if i == num_bins - 1: # Bao g·ªìm 1.0 trong bin cu·ªëi c√πng
            mask = (np.nan_to_num(rejection_scores, nan=-np.inf) >= lower_bound) & (np.nan_to_num(rejection_scores, nan=-np.inf) <= upper_bound)

        bin_samples_indices = np.where(mask)[0]
        bin_count = len(bin_samples_indices)

        if bin_count > 0:
            bin_accuracy = accuracy_score(true_labels[bin_samples_indices], model_predictions[bin_samples_indices])
            bin_rejection_score_mean = np.mean(rejection_scores[bin_samples_indices])
            ece += (bin_count / total_samples) * np.abs(bin_accuracy - bin_rejection_score_mean)
    return ece

def find_optimal_rejection_threshold(rejection_scores, original_model_predictions, true_labels, cfg):
    """
    T√¨m ng∆∞·ª°ng ƒë·ªô tin c·∫≠y t·ªëi ∆∞u tr√™n t·∫≠p validation
    ƒë·ªÉ ƒë√°p ·ª©ng ƒë·ªô ch√≠nh x√°c m·ª•c ti√™u tr√™n c√°c tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c ch·∫•p nh·∫≠n, t·ª∑ l·ªá t·ª´ ch·ªëi m·ª•c ti√™u v√† t·ªëi ∆∞u h√≥a ECE.
    """
    thresholds = np.linspace(0.0, 1.0, 1000) 
    best_threshold = 0.0
    min_deviation = float('inf')

    print("\n--- T√¨m Ng∆∞·ª°ng T·ª´ Ch·ªëi T·ªëi ∆∞u tr√™n T·∫≠p Validation ---")
    results = []
    # FIX: Added np.nan_to_num to handle potential NaNs in rejection_scores before thresholding
    rejection_scores_clean = np.nan_to_num(rejection_scores, nan=-np.inf) # Treat NaN as low score (rejected)

    for threshold in tqdm(thresholds, desc="ƒê√°nh gi√° ng∆∞·ª°ng", dynamic_ncols=True):
        # `rejection_scores` l√† ƒëi·ªÉm th·ªëng nh·∫•t, ƒëi·ªÉm cao h∆°n nghƒ©a l√† ƒë∆∞·ª£c ch·∫•p nh·∫≠n
        accepted_indices = rejection_scores_clean >= threshold 
        
        num_total = len(true_labels)
        num_accepted = np.sum(accepted_indices)
        
        current_coverage = num_accepted / num_total
        current_rejection_rate = 1.0 - current_coverage

        current_accuracy_on_accepted = 0.0
        current_ece_on_accepted = 0.0

        if num_accepted > 0:
            accepted_predictions = original_model_predictions[accepted_indices]
            accepted_true_labels = true_labels[accepted_indices]
            accepted_rejection_scores = rejection_scores[accepted_indices] # Use original scores for ECE calc

            current_accuracy_on_accepted = accuracy_score(accepted_true_labels, accepted_predictions)
            current_ece_on_accepted = calculate_ece(accepted_predictions, accepted_rejection_scores, accepted_true_labels)


        # T√≠nh ƒë·ªô l·ªách so v·ªõi m·ª•c ti√™u, s·ª≠ d·ª•ng tr·ªçng s·ªë c√≥ th·ªÉ c·∫•u h√¨nh
        accuracy_deviation = max(0, cfg.TARGET_ACCEPTED_ACCURACY - current_accuracy_on_accepted) * cfg.ACCURACY_DEVIATION_WEIGHT
        rejection_deviation = abs(current_rejection_rate - cfg.TARGET_REJECTION_RATE) * cfg.REJECTION_RATE_DEVIATION_WEIGHT
        # Gi·∫£m thi·ªÉu ECE tr√™n t·∫≠p ch·∫•p nh·∫≠n (ECE th·∫•p h∆°n l√† t·ªët h∆°n)
        ece_deviation = current_ece_on_accepted * cfg.ECE_DEVIATION_WEIGHT

        deviation = accuracy_deviation + rejection_deviation + ece_deviation

        results.append({
            'threshold': threshold,
            'accuracy_on_accepted': current_accuracy_on_accepted,
            'rejection_rate': current_rejection_rate,
            'ece_on_accepted': current_ece_on_accepted,
            'deviation': deviation
        })

    results_df = pd.DataFrame(results)
    # L·ªçc c√°c ng∆∞·ª°ng th·ª±c t·∫ø cung c·∫•p m·ªôt s·ªë ƒë·ªô ph·ªß
    results_df = results_df[results_df['rejection_rate'] < 1.0]

    if not results_df.empty:
        # T√¨m h√†ng c√≥ t·ªïng ƒë·ªô l·ªách t·ªëi thi·ªÉu
        best_row_idx = results_df['deviation'].idxmin()
        best_threshold_info = results_df.loc[best_row_idx]
        best_threshold = best_threshold_info['threshold']
        min_deviation = best_threshold_info['deviation']

        print(f"T√¨m th·∫•y ng∆∞·ª°ng t·ªëi ∆∞u: {best_threshold:.4f}")
        print(f"  ƒê·ªô ch√≠nh x√°c tr√™n c√°c tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c ch·∫•p nh·∫≠n: {best_threshold_info['accuracy_on_accepted']:.4f}")
        print(f"  T·ª∑ l·ªá t·ª´ ch·ªëi: {best_threshold_info['rejection_rate']:.4f}")
        print(f"  ECE tr√™n c√°c tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c ch·∫•p nh·∫≠n: {best_threshold_info['ece_on_accepted']:.4f}")
        print(f"  T·ªïng ƒë·ªô l·ªách: {best_threshold_info['deviation']:.4f}")
    else:
        print("Kh√¥ng th·ªÉ t√¨m th·∫•y ng∆∞·ª°ng ph√π h·ª£p, m·∫∑c ƒë·ªãnh l√† 0.5. Vui l√≤ng ki·ªÉm tra d·ªØ li·ªáu v√† m·ª•c ti√™u c·∫•u h√¨nh c·ªßa b·∫°n.")
        best_threshold = 0.5

    return best_threshold, results_df

# --- Metrics ƒê√°nh gi√° ---
def calculate_metrics(model_predictions, rejection_scores, true_labels, rejection_threshold, verbose=True):
    """
    T√≠nh to√°n c√°c ch·ªâ s·ªë ph√¢n lo·∫°i ch·ªçn l·ªçc kh√°c nhau.
    `rejection_scores` l√† ƒëi·ªÉm th·ªëng nh·∫•t, ƒëi·ªÉm cao h∆°n nghƒ©a l√† ƒë∆∞·ª£c ch·∫•p nh·∫≠n.
    """
    accepted_indices = rejection_scores >= rejection_threshold
    rejected_indices = rejection_scores < rejection_threshold

    num_total = len(true_labels)
    num_accepted = np.sum(accepted_indices)
    num_rejected = np.sum(rejected_indices)

    # ƒê·ªô ph·ªß (Coverage)
    coverage = num_accepted / num_total
    rejection_rate = num_rejected / num_total

    # ƒê·ªô ch√≠nh x√°c tr√™n c√°c Tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c Ch·∫•p nh·∫≠n (R·ªßi ro)
    if num_accepted > 0:
        accepted_predictions = model_predictions[accepted_indices]
        accepted_true_labels = true_labels[accepted_indices]
        accuracy_accepted = accuracy_score(accepted_true_labels, accepted_predictions)
        risk = 1.0 - accuracy_accepted
        # NLL v√† Brier Score tr√™n c√°c m·∫´u ƒë∆∞·ª£c ch·∫•p nh·∫≠n
        all_possible_labels = np.unique(true_labels) # Get all unique labels from the original true_labels
        nll_accepted = log_loss(accepted_true_labels, rejection_scores[accepted_indices], labels=all_possible_labels)
        brier_accepted = brier_score_loss(accepted_true_labels, rejection_scores[accepted_indices])
    else:
        accuracy_accepted = 0.0 
        risk = 1.0
        nll_accepted = np.nan
        brier_accepted = np.nan

    # ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ (ƒë·ªÉ so s√°nh)
    overall_accuracy = accuracy_score(true_labels, model_predictions)

    if verbose:
        print(f"\n--- K·∫øt qu·∫£ ƒê√°nh gi√° (Ng∆∞·ª°ng={rejection_threshold:.4f}) ---")
        print(f"ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ (T·∫•t c·∫£ c√°c m·∫´u): {overall_accuracy:.4f}")
        print(f"ƒê·ªô ph·ªß: {coverage:.4f} ({num_accepted} m·∫´u ƒë∆∞·ª£c ch·∫•p nh·∫≠n)")
        print(f"T·ª∑ l·ªá t·ª´ ch·ªëi: {rejection_rate:.4f} ({num_rejected} m·∫´u b·ªã t·ª´ ch·ªëi)")
        print(f"ƒê·ªô ch√≠nh x√°c tr√™n c√°c Tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c ch·∫•p nh·∫≠n: {accuracy_accepted:.4f}")
        print(f"R·ªßi ro tr√™n c√°c Tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c ch·∫•p nh·∫≠n: {risk:.4f}")

    # Ch·ªâ s·ªë hi·ªáu ch·ªânh (ECE - Expected Calibration Error)
    ece = calculate_ece(model_predictions, rejection_scores, true_labels)
    if verbose:
        print(f"\nExpected Calibration Error (ECE): {ece:.4f}")
        print(f"Negative Log-Likelihood (NLL) tr√™n c√°c m·∫´u ƒë∆∞·ª£c ch·∫•p nh·∫≠n: {nll_accepted:.4f}")
        print(f"Brier Score tr√™n c√°c m·∫´u ƒë∆∞·ª£c ch·∫•p nh·∫≠n: {brier_accepted:.4f}")

    # AUROC v√† AUPR calculations
    is_correct = (model_predictions == true_labels).astype(int)
    
    if len(np.unique(true_labels)) > 1: 
        fpr, tpr, roc_thresholds = roc_curve(is_correct, rejection_scores)
        auroc = auc(fpr, tpr)
        if verbose:
            print(f"AUROC (ƒêi·ªÉm t·ª´ ch·ªëi l√† ƒëi·ªÉm cho t√≠nh ƒë√∫ng ƒë·∫Øn): {auroc:.4f}")
    else:
        auroc = np.nan

    if len(np.unique(is_correct)) > 1:
        precision_correct, recall_correct, _ = precision_recall_curve(is_correct, rejection_scores)
        aupr_correct = auc(recall_correct, precision_correct)
        if verbose:
            print(f"AUPR (ƒêi·ªÉm t·ª´ ch·ªëi l√† ƒëi·ªÉm cho t√≠nh ƒë√∫ng ƒë·∫Øn): {aupr_correct:.4f}")
    else:
        aupr_correct = np.nan

    # AURC calculation
    sorted_indices = np.argsort(rejection_scores)
    sorted_predictions = model_predictions[sorted_indices]
    sorted_labels = true_labels[sorted_indices]

    risks = []
    coverages = []
    for i_idx in range(num_total):
        current_accepted_preds = sorted_predictions[i_idx:]
        current_accepted_labels = sorted_labels[i_idx:]
        current_coverage = (num_total - i_idx) / num_total

        if (num_total - i_idx) == 0:
            current_risk = 1.0 
        else:
            num_correct = np.sum(current_accepted_preds == current_accepted_labels)
            current_risk = 1.0 - (num_correct / (num_total - i_idx)) 

        coverages.append(current_coverage)
        risks.append(current_risk)

    coverages = coverages[::-1]
    risks = risks[::-1]
    aurc = np.trapz(risks, coverages)
    
    if verbose:
        print(f"Di·ªán t√≠ch d∆∞·ªõi ƒê∆∞·ªùng cong Risk-Coverage (AURC): {aurc:.4f}")

    # ‚úÖ FIXED: Added F1-Score calculation for rejection task
    # F1-Score for rejection task: ability to correctly identify rejected samples
    is_rejected = (rejection_scores < rejection_threshold).astype(int)
    is_incorrect = (model_predictions != true_labels).astype(int)
    
    if len(np.unique(is_incorrect)) > 1:
        f1_rejection = f1_score(is_incorrect, is_rejected)
        if verbose:
            print(f"F1-Score (Rejection Task - Identifying Incorrect Predictions): {f1_rejection:.4f}")
    else:
        f1_rejection = np.nan

    return {
        'overall_accuracy': overall_accuracy,
        'accuracy_on_accepted': accuracy_accepted,
        'coverage': coverage,
        'rejection_rate': rejection_rate,
        'risk': risk,
        'ece': ece,
        'nll_accepted': nll_accepted,
        'brier_accepted': brier_accepted,
        'auroc_correct_incorrect': auroc,
        'aupr_correct_incorrect': aupr_correct,
        'aurc': aurc,
        'f1_rejection': f1_rejection  # ‚úÖ FIXED: Added F1-score to returned metrics
    }


def run_baseline(config_name, mcdo_enable, label_smoothing_enable, 
                 calibration_method, final_overall_learning_metrics, ood_detection_method='none',
                 combine_ood_with_disagreement=False,
                 enable_training_dynamics=False,):
    """
    Ch·∫°y m·ªôt c·∫•u h√¨nh baseline c·ª• th·ªÉ - Updated to match main.py structure.
    """
    print(f"\\n{'='*20}\\nB·∫Øt ƒë·∫ßu ch·∫°y Baseline: {config_name}\\n{'='*20}")

    # Reset Config v·ªÅ tr·∫°ng th√°i m·∫∑c ƒë·ªãnh tr∆∞·ªõc m·ªói l·∫ßn ch·∫°y
    global cfg
    cfg = Config() 
    set_seed(cfg.RANDOM_SEED)

    # C·∫•u h√¨nh c√°c c·ªù cho baseline hi·ªán t·∫°i
    cfg.MCDO_ENABLE = mcdo_enable
    cfg.LABEL_SMOOTHING_ENABLE = label_smoothing_enable
    cfg.ENABLE_TRAINING_DYNAMICS = enable_training_dynamics

    # 3. L·∫•y D·ª± ƒëo√°n Ensemble v√† ƒêi·ªÉm T·ª´ Ch·ªëi (cho T·∫≠p Validation)
    print(f"\\nL·∫•y d·ª± ƒëo√°n v√† ƒëi·ªÉm t·ª´ ch·ªëi cho T·∫≠p Validation (S·ª≠ d·ª•ng hi·ªáu ch·ªânh: {calibration_method}, OOD: {ood_detection_method}, K·∫øt h·ª£p b·∫•t ƒë·ªìng: {combine_ood_with_disagreement})...")
    val_model_predictions, val_rejection_scores, val_true_labels, val_original_indices, _, _, _ = (
        get_rejection_scores_and_predictions(cfg, val_loader, cfg.MODEL_SAVE_DIR, 
                                            final_overall_learning_metrics, train_dataset, 
                                            calibration_method=calibration_method,
                                            ood_detection_method=ood_detection_method,
                                            combine_ood_with_disagreement=combine_ood_with_disagreement))

    # Clear CUDA cache after val predictions
    if cfg.DEVICE.type == 'cuda':
        torch.cuda.empty_cache()

    # 4. C∆° ch·∫ø T·ª´ ch·ªëi th√≠ch ·ª©ng: T√¨m ng∆∞·ª°ng t·ªëi ∆∞u tr√™n T·∫≠p Validation
    best_rejection_threshold, _ = find_optimal_rejection_threshold(
        val_rejection_scores, val_model_predictions, val_true_labels, cfg
    )
    print(f"Ng∆∞·ª°ng t·ª´ ch·ªëi cu·ªëi c√πng ƒë∆∞·ª£c ch·ªçn: {best_rejection_threshold:.4f}")

    # 5. ƒê√°nh gi√° tr√™n T·∫≠p Test b·∫±ng c√°ch s·ª≠ d·ª•ng ng∆∞·ª°ng ƒë√£ h·ªçc
    print(f"\\n--- ƒê√°nh gi√° tr√™n T·∫≠p Test (S·ª≠ d·ª•ng hi·ªáu ch·ªânh: {calibration_method}, OOD: {ood_detection_method}, K·∫øt h·ª£p b·∫•t ƒë·ªìng: {combine_ood_with_disagreement}) ---")
    (test_model_predictions, test_rejection_scores, test_true_labels, test_original_indices,
     all_ensemble_individual_probs_test, test_odin_scores, test_energy_scores) = (
        get_rejection_scores_and_predictions(cfg, test_loader, cfg.MODEL_SAVE_DIR, 
                                            final_overall_learning_metrics, train_dataset, 
                                            calibration_method=calibration_method,
                                            ood_detection_method=ood_detection_method,
                                            combine_ood_with_disagreement=combine_ood_with_disagreement))

    test_metrics = calculate_metrics(test_model_predictions, test_rejection_scores, test_true_labels, best_rejection_threshold, verbose=True)


    if hasattr(best_rejection_threshold, 'item'):
        best_rejection_threshold = best_rejection_threshold.item()
    elif hasattr(best_rejection_threshold, '__len__') and len(best_rejection_threshold) > 1:
        best_rejection_threshold = float(best_rejection_threshold)
    
    rejected_categories_info = categorize_rejected_cases(
        test_rejection_scores, test_model_predictions, test_true_labels, test_original_indices,
        best_rejection_threshold, all_ensemble_individual_probs_test, 
        all_odin_scores=test_odin_scores, all_energy_scores=test_energy_scores,
        ood_detection_method=ood_detection_method
    )


    
    print("\\n--- T√≥m t·∫Øt Ph√¢n lo·∫°i Tr∆∞·ªùng h·ª£p b·ªã T·ª´ ch·ªëi ---")
    print(f"S·ªë l∆∞·ª£ng Tr∆∞·ªùng h·ª£p T·ª´ ch·ªëi L·ªói: {len(rejected_categories_info['failure_rejection_indices'])}")
    print(f"S·ªë l∆∞·ª£ng Tr∆∞·ªùng h·ª£p T·ª´ ch·ªëi Kh√¥ng r√µ/M∆° h·ªì: {len(rejected_categories_info['unknown_ambiguous_indices'])}") 
    print(f"S·ªë l∆∞·ª£ng Tr∆∞·ªùng h·ª£p T·ª´ ch·ªëi OOD ti·ªÅm nƒÉng: {len(rejected_categories_info['potential_ood_indices'])}") 

    # 7. T·∫°o H√¨nh ·∫£nh XAI cho c√°c tr∆∞·ªùng h·ª£p quan tr·ªçng
    # loaded_ensemble_models_for_xai = []
    # for i_idx in range(cfg.NUM_ENSEMBLE_MODELS):
    #     model = BaseClassifier(num_classes=2, dropout_rate=(cfg.MCDO_DROPOUT_RATE if cfg.MCDO_ENABLE else 0.0)).to(cfg.DEVICE)
    #     model.load_state_dict(torch.load(os.path.join(cfg.MODEL_SAVE_DIR, f'best_model_ensemble_{i_idx}.pth')))
    #     model.eval() 
    #     loaded_ensemble_models_for_xai.append(model)
        
    # test_results_for_xai = {
    #     'model_preds': test_model_predictions,
    #     'rejection_scores': test_rejection_scores,
    #     'true_labels': test_true_labels,
    #     'original_indices': test_original_indices,
    #     'rejected_categories': rejected_categories_info
    # }
    
    # print(f"ƒê·∫£m b·∫£o th∆∞ m·ª•c l∆∞u XAI t·ªìn t·∫°i: {cfg.XAI_SAVE_DIR}")
    # os.makedirs(cfg.XAI_SAVE_DIR, exist_ok=True)
    # visualize_xai_examples(cfg, loaded_ensemble_models_for_xai, test_dataset, test_results_for_xai, best_rejection_threshold)

    # # ‚úÖ FIXED: 8. Tr·ª±c quan h√≥a c√°c ƒë∆∞·ªùng cong hi·ªáu su·∫•t ri√™ng cho baseline n√†y
    # print("\\n--- Tr·ª±c quan h√≥a c√°c ƒë∆∞·ªùng cong hi·ªáu su·∫•t ---")
    # plot_individual_baseline_charts(test_model_predictions, test_rejection_scores, test_true_labels, config_name, cfg.XAI_SAVE_DIR)

    # print(f"\\n{'='*20}\\nHo√†n t·∫•t ch·∫°y Baseline: {config_name}\\n{'='*20}")
    
    # ‚úÖ FIXED: Tr·∫£ v·ªÅ structure gi·ªëng main.py v·ªõi t·∫•t c·∫£ metrics c·∫ßn thi·∫øt
    return {
        'metrics': {
            'Config Name': config_name,
            'Overall Accuracy': test_metrics['overall_accuracy'],
            'Accuracy on Accepted': test_metrics['accuracy_on_accepted'],
            'Coverage': test_metrics['coverage'],
            'Rejection Rate': test_metrics['rejection_rate'],
            'Risk': test_metrics['risk'],
            'ECE': test_metrics['ece'],
            'NLL Accepted': test_metrics['nll_accepted'],
            'Brier Accepted': test_metrics['brier_accepted'],
            'AUROC': test_metrics['auroc_correct_incorrect'],
            'AUPR': test_metrics['aupr_correct_incorrect'],
            'AURC': test_metrics['aurc'],
            'F1 Rejection': test_metrics['f1_rejection'],
            'Failure Rejected Count': len(rejected_categories_info['failure_rejection_indices']),
            'Unknown/Ambiguous Rejected Count': len(rejected_categories_info['unknown_ambiguous_indices']),
            'Potential OOD Rejected Count': len(rejected_categories_info['potential_ood_indices'])
        },
        'raw_data': {
            'predictions': test_model_predictions,
            'rejection_scores': test_rejection_scores,
            'true_labels': test_true_labels
        }
    }
def categorize_rejected_cases(rejection_scores, model_predictions, true_labels, original_indices, rejection_threshold, 
                             all_ensemble_individual_probs_stacked, all_odin_scores=None, all_energy_scores=None, 
                             ood_detection_method='none'):
    """
    Ph√¢n lo·∫°i c√°c tr∆∞·ªùng h·ª£p b·ªã t·ª´ ch·ªëi.
    - 'Failure Rejection': C√°c tr∆∞·ªùng h·ª£p b·ªã t·ª´ ch·ªëi do ƒëi·ªÉm t·ª´ ch·ªëi th·∫•p V√Ä d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh kh√¥ng ch√≠nh x√°c.
    - 'Potential OOD Rejected Cases': C√°c tr∆∞·ªùng h·ª£p b·ªã t·ª´ ch·ªëi do ƒëi·ªÉm t·ª´ ch·ªëi th·∫•p V√Ä ƒë∆∞·ª£c x√°c ƒë·ªãnh l√† OOD ti·ªÅm nƒÉng.
    - 'Unknown/Ambiguous': C√°c tr∆∞·ªùng h·ª£p b·ªã t·ª´ ch·ªëi do ƒëi·ªÉm t·ª´ ch·ªëi th·∫•p, d·ª± ƒëo√°n ƒë√∫ng, v√† kh√¥ng ƒë∆∞·ª£c x√°c ƒë·ªãnh l√† OOD ti·ªÅm nƒÉng.
    """
    rejected_mask = rejection_scores < rejection_threshold
    
    rejected_indices = original_indices[rejected_mask]
    rejected_model_preds = model_predictions[rejected_mask]
    rejected_true_labels = true_labels[rejected_mask]
    rejected_rejection_scores = rejection_scores[rejected_mask]

    failure_rejection_indices = []
    unknown_ambiguous_indices = [] 
    potential_ood_indices = [] 
    
    # T·∫°o √°nh x·∫° t·ª´ ch·ªâ m·ª•c g·ªëc (to√†n c·ª•c) ƒë·∫øn v·ªã tr√≠ ph·∫≥ng c·ªßa n√≥ trong m·∫£ng `original_indices`
    original_to_flat_pos_map = {original_idx: flat_pos for flat_pos, original_idx in enumerate(original_indices)}

    print(f"\\n--- Ph√¢n lo·∫°i c√°c Tr∆∞·ªùng h·ª£p b·ªã T·ª´ ch·ªëi ({len(rejected_indices)} b·ªã t·ª´ ch·ªëi) ---")

    # If energy scores are used for OOD classification, calculate the threshold now
    energy_ood_threshold = None
    if ood_detection_method == 'energy' and all_energy_scores is not None and all_energy_scores.size > 0:
        energy_ood_threshold = np.percentile(all_energy_scores, cfg.ENERGY_CLASSIFY_PERCENTILE_THRESHOLD)
        # print(f"Ng∆∞·ª°ng ph√¢n lo·∫°i OOD Energy: {energy_ood_threshold:.4f}")

    for i_idx, rejected_original_idx in enumerate(rejected_indices):
        current_pred = rejected_model_preds[i_idx]
        current_true = rejected_true_labels[i_idx]
        current_rejection_score = rejected_rejection_scores[i_idx]

        if rejected_original_idx not in original_to_flat_pos_map:
            print(f"C·∫£nh b√°o: Ch·ªâ m·ª•c g·ªëc {rejected_original_idx} kh√¥ng t√¨m th·∫•y. B·ªè qua ph√¢n lo·∫°i cho m·∫´u n√†y.")
            continue 

        flat_pos_in_full_data = original_to_flat_pos_map[rejected_original_idx]
        
        is_potential_ood = False

        if ood_detection_method == 'odin' and all_odin_scores is not None:
            if all_odin_scores[flat_pos_in_full_data] < cfg.ODIN_CLASSIFY_THRESHOLD:
                is_potential_ood = True
        elif ood_detection_method == 'energy' and all_energy_scores is not None and energy_ood_threshold is not None:
            if all_energy_scores[flat_pos_in_full_data] < energy_ood_threshold:
                is_potential_ood = True
        else: # Default OOD classification using confidence and ensemble variance
            if all_ensemble_individual_probs_stacked.size > 0:
                individual_probs_for_this_sample = all_ensemble_individual_probs_stacked[flat_pos_in_full_data, :, :] 
                ensemble_predicted_class = current_pred
                variance_disagreement = 0.0
                if (individual_probs_for_this_sample.size > 0 and 
                    ensemble_predicted_class < individual_probs_for_this_sample.shape[1] and 
                    ensemble_predicted_class >= 0):
                    variance_disagreement = np.var(individual_probs_for_this_sample[:, ensemble_predicted_class])
                else:
                    if individual_probs_for_this_sample.size > 0:
                        variance_disagreement = np.max(np.var(individual_probs_for_this_sample, axis=0))
                    else:
                        variance_disagreement = 0.0 

                if current_rejection_score < cfg.OOD_CONFIDENCE_THRESHOLD and variance_disagreement > cfg.OOD_VARIANCE_THRESHOLD:
                    is_potential_ood = True

        # Ph√¢n lo·∫°i
        if current_pred != current_true:
            failure_rejection_indices.append(rejected_original_idx)
        elif is_potential_ood:
            potential_ood_indices.append(rejected_original_idx)
        else: # D·ª± ƒëo√°n ƒë√∫ng nh∆∞ng b·ªã t·ª´ ch·ªëi do s·ª± kh√¥ng ch·∫Øc ch·∫Øn/b·∫•t ƒë·ªìng chung trong ph√¢n ph·ªëi
            unknown_ambiguous_indices.append(rejected_original_idx)

    return {
        'rejected_data_df': pd.DataFrame({
            'original_idx': rejected_indices,
            'model_pred': rejected_model_preds,
            'true_label': rejected_true_labels,
            'rejection_score': rejected_rejection_scores
        }),
        'failure_rejection_indices': failure_rejection_indices,
        'unknown_ambiguous_indices': unknown_ambiguous_indices, 
        'potential_ood_indices': potential_ood_indices 
    }
# ‚úÖ FIXED: All plotting functions updated to match main.py

def plot_calibration_curve(model_predictions, rejection_scores, true_labels, num_bins=10, save_path=None):
    """
    V·∫Ω bi·ªÉu ƒë·ªì ƒë·ªô tin c·∫≠y (Reliability Diagram) ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng hi·ªáu ch·ªânh.
    """
    if len(rejection_scores) == 0:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì hi·ªáu ch·ªânh.")
        return

    bins = np.linspace(0., 1., num_bins + 1)
    bin_accuracies = []
    bin_rejection_scores = []
    bin_counts = []

    for i in range(num_bins):
        lower_bound = bins[i]
        upper_bound = bins[i+1]
        mask = (rejection_scores >= lower_bound) & (rejection_scores < upper_bound)
        if i == num_bins - 1:
            mask = (rejection_scores >= lower_bound) & (rejection_scores <= upper_bound)

        bin_samples_indices = np.where(mask)[0]
        bin_count = len(bin_samples_indices)

        if bin_count > 0:
            bin_accuracy = accuracy_score(true_labels[bin_samples_indices], model_predictions[bin_samples_indices])
            bin_rejection_score_mean = np.mean(rejection_scores[bin_samples_indices])
            bin_accuracies.append(bin_accuracy)
            bin_rejection_scores.append(bin_rejection_score_mean)
            bin_counts.append(bin_count)
        else:
            bin_accuracies.append(np.nan)
            bin_rejection_scores.append(np.nan)
            bin_counts.append(0)

    # L·ªçc c√°c bin r·ªóng
    valid_bins_mask = ~np.isnan(bin_accuracies)
    bin_accuracies = np.array(bin_accuracies)[valid_bins_mask]
    bin_rejection_scores = np.array(bin_rejection_scores)[valid_bins_mask]

    plt.figure(figsize=(7, 7))
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Hi·ªáu ch·ªânh ho√†n h·∫£o')
    plt.plot(bin_rejection_scores, bin_accuracies, marker='o', linestyle='-', color='blue', label='M√¥ h√¨nh')
    
    # V·∫Ω c√°c thanh bi·ªÉu ƒë·ªì
    for i_idx in range(len(bin_rejection_scores)):
        plt.plot([bin_rejection_scores[i_idx], bin_rejection_scores[i_idx]], [bin_rejection_scores[i_idx], bin_accuracies[i_idx]],
                 color='red' if bin_accuracies[i_idx] < bin_rejection_scores[i_idx] else 'green', linestyle='-', linewidth=2)

    plt.xlabel("ƒêi·ªÉm trung b√¨nh (Score)")
    plt.ylabel("ƒê·ªô ch√≠nh x√°c (Accuracy)")
    plt.title("Bi·ªÉu ƒë·ªì ƒë·ªô tin c·∫≠y (Reliability Diagram)")
    plt.grid(True)
    plt.legend()
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    if save_path:
        plt.savefig(save_path)
        print(f"Bi·ªÉu ƒë·ªì ƒë·ªô tin c·∫≠y ƒë√£ l∆∞u v√†o: {save_path}")
    plt.show()
    plt.close()

def plot_roc_curve(model_predictions, rejection_scores, true_labels, save_path=None):
    """
    V·∫Ω ƒë∆∞·ªùng cong ROC (Receiver Operating Characteristic).
    """
    if len(rejection_scores) == 0:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong ROC.")
        return

    is_correct = (model_predictions == true_labels).astype(int)
    
    if len(np.unique(is_correct)) < 2:
        print("Kh√¥ng ƒë·ªß bi·∫øn th·ªÉ trong c√°c d·ª± ƒëo√°n ƒë√∫ng/sai ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong ROC.")
        return

    fpr, tpr, thresholds = roc_curve(is_correct, rejection_scores)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ƒê∆∞·ªùng cong ROC (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Ng·∫´u nhi√™n')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('T·ª∑ l·ªá d∆∞∆°ng t√≠nh gi·∫£ (False Positive Rate)')
    plt.ylabel('T·ª∑ l·ªá d∆∞∆°ng t√≠nh th·∫≠t (True Positive Rate)')
    plt.title('ƒê∆∞·ªùng cong ROC')
    plt.legend(loc="lower right")
    plt.grid(True)
    if save_path:
        plt.savefig(save_path)
        print(f"ƒê∆∞·ªùng cong ROC ƒë√£ l∆∞u v√†o: {save_path}")
    plt.show()
    plt.close()

def plot_pr_curve(model_predictions, rejection_scores, true_labels, save_path=None):
    """
    V·∫Ω ƒë∆∞·ªùng cong PR (Precision-Recall).
    """
    if len(rejection_scores) == 0:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong PR.")
        return

    is_correct = (model_predictions == true_labels).astype(int)

    if len(np.unique(is_correct)) < 2:
        print("Kh√¥ng ƒë·ªß bi·∫øn th·ªÉ trong c√°c d·ª± ƒëo√°n ƒë√∫ng/sai ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong PR.")
        return

    precision, recall, thresholds = precision_recall_curve(is_correct, rejection_scores)
    pr_auc = auc(recall, precision)

    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='purple', lw=2, label=f'ƒê∆∞·ªùng cong PR (AUC = {pr_auc:.2f})')
    plt.xlabel('ƒê·ªô thu h·ªìi (Recall)')
    plt.ylabel('ƒê·ªô ch√≠nh x√°c (Precision)')
    plt.title('ƒê∆∞·ªùng cong Precision-Recall')
    plt.legend(loc="lower left")
    plt.grid(True)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    if save_path:
        plt.savefig(save_path)
        print(f"ƒê∆∞·ªùng cong PR ƒë√£ l∆∞u v√†o: {save_path}")
    plt.show()
    plt.close()

# ‚úÖ FIXED: Individual plotting functions for each baseline
def plot_individual_baseline_charts(model_predictions, rejection_scores, true_labels, config_name, save_dir):
    """
    Plot individual performance charts for a single baseline.
    """
    # Create subdirectory for this baseline
    baseline_dir = os.path.join(save_dir, config_name.replace(' ', '_').replace('/', '_'))
    os.makedirs(baseline_dir, exist_ok=True)
    
    print(f"üîÑ T·∫°o bi·ªÉu ƒë·ªì cho {config_name}...")
    
    # Reliability Diagram
    plot_calibration_curve(model_predictions, rejection_scores, true_labels, 
                          save_path=os.path.join(baseline_dir, f'reliability_diagram_{config_name.replace(" ", "_")}.png'))
    
    # ROC Curve
    plot_roc_curve(model_predictions, rejection_scores, true_labels, 
                  save_path=os.path.join(baseline_dir, f'roc_curve_{config_name.replace(" ", "_")}.png'))
    
    # PR Curve
    plot_pr_curve(model_predictions, rejection_scores, true_labels, 
                 save_path=os.path.join(baseline_dir, f'pr_curve_{config_name.replace(" ", "_")}.png'))

# ‚úÖ FIXED: Comprehensive comparison plotting to match main.py
def plot_all_calibration_curves(all_results, save_dir, num_bins=10):
    """
    V·∫Ω bi·ªÉu ƒë·ªì ƒë·ªô tin c·∫≠y cho t·∫•t c·∫£ c√°c baseline tr√™n c√πng m·ªôt h√¨nh.

    Args:
        all_results (list): Danh s√°ch c√°c dictionary t·ª´ run_baseline, m·ªói dictionary ch·ª©a 'metrics', 'test_predictions', 'test_rejection_scores', 'test_labels'.
        save_dir (str): Th∆∞ m·ª•c ƒë·ªÉ l∆∞u bi·ªÉu ƒë·ªì t·ªïng h·ª£p.
        num_bins (int): S·ªë l∆∞·ª£ng bin cho bi·ªÉu ƒë·ªì ƒë·ªô tin c·∫≠y. M·∫∑c ƒë·ªãnh l√† 10.
    """
    plt.figure(figsize=(20, 10))
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Hi·ªáu ch·ªânh ho√†n h·∫£o')

    for result in all_results:
        try:
            config_name = result['metrics']['Baseline']
            preds = result['test_predictions']
            rejection_scores = result['test_rejection_scores']
            labels = result['test_labels']
        except KeyError as e:
            print(f"L·ªói: Kh√¥ng t√¨m th·∫•y key {e} trong result cho baseline.")
            continue

        if not all(isinstance(arr, np.ndarray) for arr in [preds, rejection_scores, labels]):
            print(f"L·ªói: D·ªØ li·ªáu ƒë·∫ßu v√†o cho {config_name} ph·∫£i l√† np.ndarray.")
            continue

        if len(rejection_scores) == 0:
            print(f"Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªô tin c·∫≠y ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ƒë·ªô tin c·∫≠y cho {config_name}.")
            continue

        bins = np.linspace(0., 1., num_bins + 1)
        bin_accuracies = []
        bin_rejection_scores = []

        for i in range(num_bins):
            lower_bound = bins[i]
            upper_bound = bins[i + 1]
            mask = (rejection_scores >= lower_bound) & (rejection_scores < upper_bound)
            if i == num_bins - 1:
                mask = (rejection_scores >= lower_bound) & (rejection_scores <= upper_bound)

            bin_indices = np.where(mask)[0]
            if len(bin_indices) > 0:
                bin_accuracy = accuracy_score(labels[bin_indices], preds[bin_indices])
                bin_rejection_score_mean = np.mean(rejection_scores[bin_indices])
                bin_accuracies.append(bin_accuracy)
                bin_rejection_scores.append(bin_rejection_score_mean)
            else:
                bin_accuracies.append(np.nan)
                bin_rejection_scores.append(np.nan)

        valid_bins_mask = ~np.isnan(bin_accuracies)
        bin_accuracies = np.array(bin_accuracies)[valid_bins_mask]
        bin_rejection_scores = np.array(bin_rejection_scores)[valid_bins_mask]

        plt.plot(bin_rejection_scores, bin_accuracies, marker='o', linestyle='-', label=config_name)

    plt.xlabel("ƒêi·ªÉm trung b√¨nh (Score)")
    plt.ylabel("ƒê·ªô ch√≠nh x√°c (Accuracy)")
    plt.title("Bi·ªÉu ƒë·ªì ƒê·ªô tin c·∫≠y cho c√°c Baseline")
    plt.grid(True)
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'all_reliability_diagrams.png')
    plt.savefig(save_path, bbox_inches='tight')
    print(f"ƒê√£ l∆∞u Bi·ªÉu ƒë·ªì ƒê·ªô tin c·∫≠y t·ªïng h·ª£p v√†o: {save_path}")
    plt.show()
    plt.close()


def plot_all_roc_curves(all_results, save_dir):
    """
    Plots ROC curves for all baselines on a single figure.
    """
    plt.figure(figsize=(10, 8))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Ng·∫´u nhi√™n')

    for result in all_results:
        config_name = result['metrics']['Config Name']
        preds = result['raw_data']['predictions']
        rejection_scores = result['raw_data']['rejection_scores']
        labels = result['raw_data']['true_labels']

        if len(rejection_scores) == 0:
            print(f"Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong ROC cho {config_name}.")
            continue
        
        is_correct = (preds == labels).astype(int)
        if len(np.unique(is_correct)) < 2:
            print(f"Kh√¥ng ƒë·ªß bi·∫øn th·ªÉ ƒë√∫ng/sai ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong ROC cho {config_name}.")
            continue

        fpr, tpr, _ = roc_curve(is_correct, rejection_scores)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'{config_name} (AUC = {roc_auc:.2f})')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('T·ª∑ l·ªá d∆∞∆°ng t√≠nh gi·∫£ (False Positive Rate)')
    plt.ylabel('T·ª∑ l·ªá d∆∞∆°ng t√≠nh th·∫≠t (True Positive Rate)')
    plt.title('ƒê∆∞·ªùng cong ROC cho c√°c Baseline')
    plt.legend(loc="lower right", bbox_to_anchor=(1, 0))
    plt.grid(True)
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'all_roc_curves.png')
    plt.savefig(save_path)
    print(f"ƒê√£ l∆∞u ƒê∆∞·ªùng cong ROC t·ªïng h·ª£p v√†o: {save_path}")
    plt.show()
    plt.close()

def plot_all_pr_curves(all_results, save_dir):
    """
    Plots Precision-Recall curves for all baselines on a single figure.
    """
    plt.figure(figsize=(10, 8))

    for result in all_results:
        config_name = result['metrics']['Config Name']
        preds = result['raw_data']['predictions']
        rejection_scores = result['raw_data']['rejection_scores']
        labels = result['raw_data']['true_labels']

        if len(rejection_scores) == 0:
            print(f"Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong PR cho {config_name}.")
            continue
        
        is_correct = (preds == labels).astype(int)
        if len(np.unique(is_correct)) < 2:
            print(f"Kh√¥ng ƒë·ªß bi·∫øn th·ªÉ ƒë√∫ng/sai ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong PR cho {config_name}.")
            continue

        precision, recall, _ = precision_recall_curve(is_correct, rejection_scores)
        pr_auc = auc(recall, precision)
        plt.plot(recall, precision, lw=2, label=f'{config_name} (AUC = {pr_auc:.2f})')

    plt.xlabel('ƒê·ªô thu h·ªìi (Recall)')
    plt.ylabel('ƒê·ªô ch√≠nh x√°c (Precision)')
    plt.title('ƒê∆∞·ªùng cong Precision-Recall cho c√°c Baseline')
    plt.legend(loc="lower left", bbox_to_anchor=(0, 0))
    plt.grid(True)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'all_pr_curves.png')
    plt.savefig(save_path)
    print(f"ƒê√£ l∆∞u ƒê∆∞·ªùng cong Precision-Recall t·ªïng h·ª£p v√†o: {save_path}")
    plt.show()
    plt.close()

def plot_roc_curve(model_predictions, rejection_scores, true_labels, save_path=None):
    """
    V·∫Ω ƒë∆∞·ªùng cong ROC (Receiver Operating Characteristic).
    S·ª≠ d·ª•ng 'rejection_scores' (ƒëi·ªÉm t·ª´ ch·ªëi ho·∫∑c ƒë·ªô tin c·∫≠y ƒë√£ x·ª≠ l√Ω) l√†m ƒëi·ªÉm s·ªë ƒë·ªÉ ph√¢n bi·ªát ƒë√∫ng/sai.
    """
    if len(rejection_scores) == 0:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong ROC.")
        return

    is_correct = (model_predictions == true_labels).astype(int)
    
    if len(np.unique(is_correct)) < 2:
        print("Kh√¥ng ƒë·ªß bi·∫øn th·ªÉ trong c√°c d·ª± ƒëo√°n ƒë√∫ng/sai ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong ROC.")
        return

    fpr, tpr, thresholds = roc_curve(is_correct, rejection_scores)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ƒê∆∞·ªùng cong ROC (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Ng·∫´u nhi√™n')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('T·ª∑ l·ªá d∆∞∆°ng t√≠nh gi·∫£ (False Positive Rate)')
    plt.ylabel('T·ª∑ l·ªá d∆∞∆°ng t√≠nh th·∫≠t (True Positive Rate)')
    plt.title('ƒê∆∞·ªùng cong ƒë·∫∑c tr∆∞ng ho·∫°t ƒë·ªông c·ªßa b·ªô thu (ROC Curve)')
    plt.legend(loc="lower right")
    plt.grid(True)
    if save_path:
        plt.savefig(save_path)
        print(f"ƒê∆∞·ªùng cong ROC ƒë√£ l∆∞u v√†o: {save_path}")
    plt.show()
    plt.close()

def plot_pr_curve(model_predictions, rejection_scores, true_labels, save_path=None):
    """
    V·∫Ω ƒë∆∞·ªùng cong PR (Precision-Recall).
    """
    if len(rejection_scores) == 0:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong PR.")
        return

    is_correct = (model_predictions == true_labels).astype(int)

    if len(np.unique(is_correct)) < 2:
        print("Kh√¥ng ƒë·ªß bi·∫øn th·ªÉ trong c√°c d·ª± ƒëo√°n ƒë√∫ng/sai ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong PR.")
        return

    precision, recall, thresholds = precision_recall_curve(is_correct, rejection_scores)
    pr_auc = auc(recall, precision)

    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='purple', lw=2, label=f'ƒê∆∞·ªùng cong PR (AUC = {pr_auc:.2f})')
    plt.xlabel('ƒê·ªô thu h·ªìi (Recall)')
    plt.ylabel('ƒê·ªô ch√≠nh x√°c (Precision)')
    plt.title('ƒê∆∞·ªùng cong ch√≠nh x√°c-ƒë·ªô thu h·ªìi (Precision-Recall Curve)')
    plt.legend(loc="lower left")
    plt.grid(True)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    if save_path:
        plt.savefig(save_path)
        print(f"ƒê∆∞·ªùng cong PR ƒë√£ l∆∞u v√†o: {save_path}")
    plt.show()
    plt.close()
def plot_individual_baseline_charts(model_predictions, rejection_scores, true_labels, baseline_name, save_dir):
    """
    T·∫°o c√°c bi·ªÉu ƒë·ªì ph√¢n t√≠ch performance cho m·ªôt baseline ri√™ng l·∫ª.
    """
    print(f"üìä Creating performance charts for {baseline_name}...")
    
    # Create subfolder for this baseline
    baseline_charts_dir = os.path.join(save_dir, f"{baseline_name.replace(' ', '_')}_charts")
    os.makedirs(baseline_charts_dir, exist_ok=True)
    
    # 1. Calibration Reliability Diagram
    plt.figure(figsize=(10, 6))
    
    # Calculate bins for calibration
    n_bins = 10
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    accuracies = []
    confidences = []
    
    is_correct = (model_predictions == true_labels).astype(float)
    
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (rejection_scores > bin_lower) & (rejection_scores <= bin_upper)
        prop_in_bin = in_bin.mean()
        
        if prop_in_bin > 0:
            accuracy_in_bin = is_correct[in_bin].mean()
            avg_confidence_in_bin = rejection_scores[in_bin].mean()
            accuracies.append(accuracy_in_bin)
            confidences.append(avg_confidence_in_bin)
        else:
            accuracies.append(0)
            confidences.append((bin_lower + bin_upper) / 2)
    
    plt.subplot(1, 2, 1)
    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
    plt.plot(confidences, accuracies, 'ro-', label=f'{baseline_name}')
    plt.xlabel('Mean Predicted Confidence')
    plt.ylabel('Accuracy')
    plt.title('Reliability Diagram')
    plt.legend()
    plt.grid(True)
    
    # 2. Confidence Histogram
    plt.subplot(1, 2, 2)
    plt.hist(rejection_scores, bins=20, alpha=0.7, edgecolor='black')
    plt.xlabel('Confidence Score')
    plt.ylabel('Frequency')
    plt.title('Confidence Distribution')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(baseline_charts_dir, f'{baseline_name}_calibration.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # 3. Risk-Coverage Curve
    plt.figure(figsize=(8, 6))
    
    # Sort by confidence (descending)
    sorted_indices = np.argsort(rejection_scores)[::-1]
    sorted_predictions = model_predictions[sorted_indices]
    sorted_labels = true_labels[sorted_indices]
    
    coverages = []
    risks = []
    
    n_samples = len(sorted_predictions)
    for i in range(n_samples):
        coverage = (i + 1) / n_samples
        accepted_preds = sorted_predictions[:i+1]
        accepted_labels = sorted_labels[:i+1]
        
        if len(accepted_preds) > 0:
            accuracy = np.mean(accepted_preds == accepted_labels)
            risk = 1 - accuracy
        else:
            risk = 1.0
            
        coverages.append(coverage)
        risks.append(risk)
    
    plt.plot(coverages, risks, 'b-', linewidth=2, label=f'{baseline_name}')
    plt.xlabel('Coverage')
    plt.ylabel('Risk')
    plt.title('Risk-Coverage Curve')
    plt.grid(True)
    plt.legend()
    
    plt.savefig(os.path.join(baseline_charts_dir, f'{baseline_name}_risk_coverage.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # 4. ROC Curve for Correctness Prediction
    plt.figure(figsize=(8, 6))
    
    if len(np.unique(is_correct)) > 1:
        fpr, tpr, _ = roc_curve(is_correct, rejection_scores)
        roc_auc = auc(fpr, tpr)
        
        plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'{baseline_name} (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve - Correctness Prediction')
        plt.legend()
        plt.grid(True)
    else:
        plt.text(0.5, 0.5, 'Cannot compute ROC\n(only one class)', 
                horizontalalignment='center', verticalalignment='center', 
                transform=plt.gca().transAxes, fontsize=14)
    
    plt.savefig(os.path.join(baseline_charts_dir, f'{baseline_name}_roc.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"‚úÖ Charts saved to {baseline_charts_dir}")
    return baseline_charts_dir
def plot_risk_coverage_curve(model_predictions, rejection_scores, true_labels, save_path=None):
    """
    V·∫Ω ƒë∆∞·ªùng cong Risk-Coverage.
    """
    if len(rejection_scores) == 0:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng cong Risk-Coverage.")
        return

    sorted_indices = np.argsort(rejection_scores)
    sorted_predictions = model_predictions[sorted_indices]
    sorted_labels = true_labels[sorted_indices]

    risks = []
    coverages = []
    num_total = len(true_labels)

    for i_idx in range(num_total):
        current_accepted_preds = sorted_predictions[i_idx:]
        current_accepted_labels = sorted_labels[i_idx:]
        current_coverage = (num_total - i_idx) / num_total

        if (num_total - i_idx) == 0:
            current_risk = 1.0  
        else:
            num_correct = np.sum(current_accepted_preds == current_accepted_labels)
            current_risk = 1.0 - (num_correct / (num_total - i_idx)) 

        coverages.append(current_coverage)
        risks.append(current_risk)

    # ƒê·∫£o ng∆∞·ª£c ƒë·ªÉ c√≥ coverage t·ª´ th·∫•p ƒë·∫øn cao
    coverages = coverages[::-1]
    risks = risks[::-1]

    plt.figure(figsize=(8, 6))
    plt.plot(coverages, risks, color='red', lw=2, marker='o', markersize=2, label='ƒê∆∞·ªùng cong Risk-Coverage')
    plt.xlabel('ƒê·ªô ph·ªß (Coverage)')
    plt.ylabel('R·ªßi ro (Risk)')
    plt.title('ƒê∆∞·ªùng cong Risk-Coverage')
    plt.legend()
    plt.grid(True)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    if save_path:
        plt.savefig(save_path)
        print(f"ƒê∆∞·ªùng cong Risk-Coverage ƒë√£ l∆∞u v√†o: {save_path}")
    plt.show()
    plt.close()

def plot_comparison_metrics(all_baseline_results, save_path=None):
    """
    V·∫Ω bi·ªÉu ƒë·ªì so s√°nh gi·ªØa t·∫•t c·∫£ c√°c baseline.
    """
    if not all_baseline_results:
        print("Kh√¥ng c√≥ k·∫øt qu·∫£ baseline ƒë·ªÉ so s√°nh.")
        return

    baseline_names = list(all_baseline_results.keys())
    metrics_to_compare = ['accuracy_on_accepted', 'coverage', 'ece', 'auroc_correct_incorrect', 'aurc', 'f1_rejection']  # ‚úÖ FIXED: Added f1_rejection

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()

    for i_idx, metric in enumerate(metrics_to_compare):
        if i_idx >= len(axes):
            break
        
        values = [all_baseline_results[baseline].get(metric, np.nan) for baseline in baseline_names]
        valid_data = [(name, val) for name, val in zip(baseline_names, values) if not np.isnan(val)]
        
        if valid_data:
            names, vals = zip(*valid_data)
            ax = axes[i_idx]
            bars = ax.bar(range(len(names)), vals, color=plt.cm.Set3(np.linspace(0, 1, len(names))))
            ax.set_xlabel('Baseline')
            ax.set_ylabel(metric.replace('_', ' ').title())
            ax.set_title(f'So s√°nh {metric.replace("_", " ").title()}')
            ax.set_xticks(range(len(names)))
            ax.set_xticklabels(names, rotation=45, ha='right')
            ax.grid(True, alpha=0.3)
            
            # Th√™m gi√° tr·ªã tr√™n ƒë·∫ßu c√°c thanh
            for j_idx, (bar, val) in enumerate(zip(bars, vals)):
                ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                       f'{val:.3f}', ha='center', va='bottom', fontsize=9)
        else:
            axes[i_idx].text(0.5, 0.5, f'Kh√¥ng c√≥ d·ªØ li·ªáu cho {metric}', 
                           ha='center', va='center', transform=axes[i_idx].transAxes)

    # ·∫®n subplot cu·ªëi c√πng n·∫øu kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng
    if len(metrics_to_compare) < len(axes):
        axes[-1].axis('off')

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Bi·ªÉu ƒë·ªì so s√°nh ƒë√£ l∆∞u v√†o: {save_path}")
    plt.show()
    plt.close()

def plot_risk_coverage_comparison(all_baseline_results, all_baseline_test_results, save_path=None):
    """
    V·∫Ω so s√°nh ƒë∆∞·ªùng cong Risk-Coverage cho t·∫•t c·∫£ c√°c baseline.
    """
    plt.figure(figsize=(12, 8))
    
    colors = plt.cm.Set1(np.linspace(0, 1, len(all_baseline_test_results)))
    
    for i_idx, (baseline_name, test_results) in enumerate(all_baseline_test_results.items()):
        model_preds = test_results['model_preds']
        rejection_scores = test_results['rejection_scores']
        true_labels = test_results['true_labels']
        
        if len(rejection_scores) == 0:
            continue
            
        sorted_indices = np.argsort(rejection_scores)
        sorted_predictions = model_preds[sorted_indices]
        sorted_labels = true_labels[sorted_indices]

        risks = []
        coverages = []
        num_total = len(true_labels)

        for j_idx in range(num_total):
            current_accepted_preds = sorted_predictions[j_idx:]
            current_accepted_labels = sorted_labels[j_idx:]
            current_coverage = (num_total - j_idx) / num_total

            if (num_total - j_idx) == 0:
                current_risk = 1.0  
            else:
                num_correct = np.sum(current_accepted_preds == current_accepted_labels)
                current_risk = 1.0 - (num_correct / (num_total - j_idx)) 

            coverages.append(current_coverage)
            risks.append(current_risk)

        coverages = coverages[::-1]
        risks = risks[::-1]
        
        plt.plot(coverages, risks, color=colors[i_idx], lw=2, label=baseline_name, alpha=0.8)

    plt.xlabel('ƒê·ªô ph·ªß (Coverage)')
    plt.ylabel('R·ªßi ro (Risk)')
    plt.title('So s√°nh ƒê∆∞·ªùng cong Risk-Coverage c·ªßa t·∫•t c·∫£ Baseline')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"So s√°nh ƒë∆∞·ªùng cong Risk-Coverage ƒë√£ l∆∞u v√†o: {save_path}")
    plt.show()
    plt.close()

def parse_args():
    parser = argparse.ArgumentParser(description='Run selective classification on medical imaging datasets.')
    parser.add_argument('--dataset', type=str, default='covidqu_xray',
                        choices=list(cfg.DATASETS.keys()),
                        help='Dataset to use for training and evaluation.')
    return parser.parse_args()

import multiprocessing
import subprocess
import zipfile

def download_and_extract_dataset(dataset_config):
    """T·∫£i xu·ªëng v√† gi·∫£i n√©n t·∫≠p d·ªØ li·ªáu t·ª´ Kaggle."""
    zip_path = dataset_config['zip_path']
    extract_path = dataset_config['extract_path']
    curl_command = dataset_config['curl']

    # T·∫£i xu·ªëng
    print(f"Downloading dataset: {dataset_config['name']}...")
    subprocess.run(curl_command, shell=True, check=True)

    # Gi·∫£i n√©n
    print(f"Extracting dataset to {extract_path}...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    
    # X√≥a file zip ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng
    os.remove(zip_path)
    print(f"Dataset extracted and zip file removed.")


if __name__ == "__main__":   
    multiprocessing.freeze_support() 
    print("üöÄ Train 1 Ensemble duy nh·∫•t r·ªìi ƒë√°nh gi√° c√°c baseline")
    cfg = Config()
    args = parse_args()
    selected_dataset = args.dataset
    print(f"üöÄ Running with dataset: {cfg.DATASETS[selected_dataset]['name']}")
    download_and_extract_dataset(cfg.DATASETS[selected_dataset])
    cfg.ODIN_TEMP = 1000.0
    cfg.ODIN_EPSILON = 0.0014
    cfg.NUM_DROPOUT_PASSES = 1
    cfg.MCDO_ENABLE = True
    cfg.LABEL_SMOOTHING_ENABLE = True
    cfg.ENABLE_TRAINING_DYNAMICS = True

    # T·∫£i d·ªØ li·ªáu
    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = prepare_datasets(cfg, selected_dataset)

    # Train
    overall_learning_metrics = train_ensemble(cfg, train_loader, val_loader)

    all_baseline_results = []
        
    baselines_to_run = [
        {
            'config_name': 'Baseline 0 - Current Ensemble (Temperature Scaling)',
            'mcdo_enable': False,
            'calibration_method': 'temperature_scaling',
            'ood_detection_method': 'none',
            'combine_ood_with_disagreement': False,
            'enable_training_dynamics': False
        },
        {
            'config_name': 'Baseline A.1 - Ensemble + MCDO',
            'mcdo_enable': True, # B·∫≠t MCDO cho suy lu·∫≠n
            'calibration_method': 'temperature_scaling',
            'ood_detection_method': 'none',
            'combine_ood_with_disagreement': False,
            'enable_training_dynamics': False
        },
        {
            'config_name': 'Baseline A.2.1 - Isotonic Regression',
            'mcdo_enable': False,
            'calibration_method': 'isotonic_regression',
            'ood_detection_method': 'none',
            'combine_ood_with_disagreement': False,
            'enable_training_dynamics': False
        },
        {
            'config_name': 'Baseline A.2.2 - Beta Calibration',
            'mcdo_enable': False,
            'calibration_method': 'beta_calibration',
            'ood_detection_method': 'none',
            'combine_ood_with_disagreement': False,
            'enable_training_dynamics': False
        },
        {
            'config_name': 'Baseline B.1.1 - Ensemble + ODIN (Basic)',
            'mcdo_enable': False,
            'calibration_method': 'temperature_scaling',
            'ood_detection_method': 'odin',
            'combine_ood_with_disagreement': False,
            'enable_training_dynamics': False
        },
        {
            'config_name': 'Baseline B.1.2 - Ensemble + ODIN (Combined)',
            'mcdo_enable': False,
            'calibration_method': 'temperature_scaling',
            'ood_detection_method': 'odin',
            'combine_ood_with_disagreement': True,
            'enable_training_dynamics': False
        },
        {
            'config_name': 'Baseline B.2.1 - Ensemble + Energy Score (Basic)',
            'mcdo_enable': False,
            'calibration_method': 'temperature_scaling',
            'ood_detection_method': 'energy', # Thay ƒë·ªïi th√†nh 'energy'
            'combine_ood_with_disagreement': False,
            'enable_training_dynamics': False
        },
        {
            'config_name': 'Baseline B.2.2 - Ensemble + Energy Score (Combined)',
            'mcdo_enable': False,
            'calibration_method': 'temperature_scaling',
            'ood_detection_method': 'energy', # Thay ƒë·ªïi th√†nh 'energy'
            'combine_ood_with_disagreement': True,
            'enable_training_dynamics': False
        },
        {
            'config_name': 'Baseline B.3 - Ensemble + Training Dynamics Insights',
            'mcdo_enable': False,
            'calibration_method': 'temperature_scaling',
            'ood_detection_method': 'none',
            'combine_ood_with_disagreement': False,
            'enable_training_dynamics': True # B·∫≠t ph√¢n t√≠ch ƒë·ªông l·ª±c hu·∫•n luy·ªán
        }
    ]
